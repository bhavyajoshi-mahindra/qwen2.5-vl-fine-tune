{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "3o1EVDlwMYra",
        "qrWl4wMKViOv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6203193365264626b3fed486ea0d9a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5396b20045541f4bfe1b2b230f34131",
              "IPY_MODEL_56b64c4c95794fdc84f536f79bb864f9",
              "IPY_MODEL_23d42eb3cd5d43c79815de655c3e1ef4"
            ],
            "layout": "IPY_MODEL_4a37d9960b2d44a787d38a7972355830"
          }
        },
        "e5396b20045541f4bfe1b2b230f34131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e34262023d447fab5247133d8a2aa7a",
            "placeholder": "​",
            "style": "IPY_MODEL_d563827595f44e958850d4bd58154b19",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "56b64c4c95794fdc84f536f79bb864f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3440bc765014c8eb6755c6f3f923d67",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7c77deeb2a44f0eb1c39e4aa56ceb3f",
            "value": 4
          }
        },
        "23d42eb3cd5d43c79815de655c3e1ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61860e794f074a3abff4c0df13c5d789",
            "placeholder": "​",
            "style": "IPY_MODEL_3a0153f03b654436b4cb9ef2f0c3d942",
            "value": " 4/4 [00:00&lt;00:00,  8.55it/s]"
          }
        },
        "4a37d9960b2d44a787d38a7972355830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e34262023d447fab5247133d8a2aa7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d563827595f44e958850d4bd58154b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3440bc765014c8eb6755c6f3f923d67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7c77deeb2a44f0eb1c39e4aa56ceb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61860e794f074a3abff4c0df13c5d789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a0153f03b654436b4cb9ef2f0c3d942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ef13a50f3a54b8c8ce3430e66fbb5f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7778c0a3a37b423393eaf624ab53a1df",
              "IPY_MODEL_8a794ac5f79b4ce281bc14ded2abf9fa",
              "IPY_MODEL_4363529174364421809643075f82c046"
            ],
            "layout": "IPY_MODEL_aa3d58de94f84587889756b37816edda"
          }
        },
        "7778c0a3a37b423393eaf624ab53a1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1d6a71abda94aec8143fbad19080dd7",
            "placeholder": "​",
            "style": "IPY_MODEL_c90bc9bff3d24d579e4e5f8d9b57e6d1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8a794ac5f79b4ce281bc14ded2abf9fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6415d38efe044ed8cd79a6293cd1e39",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29829c8949d142bc9af916c33241129b",
            "value": 4
          }
        },
        "4363529174364421809643075f82c046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8544f7400e954b43b9f0d3bb496f15ad",
            "placeholder": "​",
            "style": "IPY_MODEL_b5112783fa9c428b88b9c90bcc6795c9",
            "value": " 4/4 [00:00&lt;00:00,  8.33it/s]"
          }
        },
        "aa3d58de94f84587889756b37816edda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1d6a71abda94aec8143fbad19080dd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c90bc9bff3d24d579e4e5f8d9b57e6d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6415d38efe044ed8cd79a6293cd1e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29829c8949d142bc9af916c33241129b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8544f7400e954b43b9f0d3bb496f15ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5112783fa9c428b88b9c90bcc6795c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf40e291507c4803a83505afb67c18f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac77aa0c58594fc398756c394702c549",
              "IPY_MODEL_5b57f2a739ef4ec2a9165f1f07c8c835",
              "IPY_MODEL_57e419b77f174a87a90092827d3658cf"
            ],
            "layout": "IPY_MODEL_0aff0eae6a584d6e8dc867e305b3b9df"
          }
        },
        "ac77aa0c58594fc398756c394702c549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6603dd5266946ea9b74f3d9ece5c9ff",
            "placeholder": "​",
            "style": "IPY_MODEL_3bc01c933fd64de0870577cbff1c74d2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5b57f2a739ef4ec2a9165f1f07c8c835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59f4f5d7523545569f1ec2c48554b35f",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c77af80aec846aea5b9561411726c62",
            "value": 5
          }
        },
        "57e419b77f174a87a90092827d3658cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e55ece055b64565ba6aba7c7b5ced52",
            "placeholder": "​",
            "style": "IPY_MODEL_25f949a2ec4d4e4fa47d606d320d1a27",
            "value": " 5/5 [02:12&lt;00:00, 22.78s/it]"
          }
        },
        "0aff0eae6a584d6e8dc867e305b3b9df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6603dd5266946ea9b74f3d9ece5c9ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bc01c933fd64de0870577cbff1c74d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59f4f5d7523545569f1ec2c48554b35f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c77af80aec846aea5b9561411726c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e55ece055b64565ba6aba7c7b5ced52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f949a2ec4d4e4fa47d606d320d1a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c446a8af867a4bab86942978f3b67408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a661da9178774e4e927e83c2b3a1e745",
              "IPY_MODEL_2cd565ce055144be8adf9d33cfccb2da",
              "IPY_MODEL_fcffc5c564a74a9a9f576815528d0ec2"
            ],
            "layout": "IPY_MODEL_f7d7b628b9864cdf97a51f6e36846987"
          }
        },
        "a661da9178774e4e927e83c2b3a1e745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f71b2d93594be0854d032156a67a8d",
            "placeholder": "​",
            "style": "IPY_MODEL_3c70f31ce71d4176ad4af2f5a29063d7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2cd565ce055144be8adf9d33cfccb2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44941ed6848948da8949b989cd1abf2a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce4a41da44054e4aabac58d611e70380",
            "value": 4
          }
        },
        "fcffc5c564a74a9a9f576815528d0ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51d8cad0288d4b51b02a99b94dc1829e",
            "placeholder": "​",
            "style": "IPY_MODEL_7c68506d6e4f4de99f8d80f6377d86fa",
            "value": " 4/4 [02:02&lt;00:00, 27.09s/it]"
          }
        },
        "f7d7b628b9864cdf97a51f6e36846987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f71b2d93594be0854d032156a67a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c70f31ce71d4176ad4af2f5a29063d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44941ed6848948da8949b989cd1abf2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce4a41da44054e4aabac58d611e70380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51d8cad0288d4b51b02a99b94dc1829e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c68506d6e4f4de99f8d80f6377d86fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c18f5b9fc744767a6a6fe754f47254c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ff58536d5d140a78543e550ab2338fe",
              "IPY_MODEL_614e4844b4f2403bb7db01ce41dc7e47",
              "IPY_MODEL_78c7658e7f514275ad861272c69aebbe"
            ],
            "layout": "IPY_MODEL_27c5fc172dc74b509592d3f6ea88355b"
          }
        },
        "9ff58536d5d140a78543e550ab2338fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3292274ae5a749758efd4081f9b18e50",
            "placeholder": "​",
            "style": "IPY_MODEL_b218f8b9879f4c87b03d1e23fbdfb5d7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "614e4844b4f2403bb7db01ce41dc7e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3472915729914fda80ce2a5632a56f24",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_548fc4124532457d867594d57fac9d4a",
            "value": 5
          }
        },
        "78c7658e7f514275ad861272c69aebbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7094db87f924830af9bffdbbfaa97fd",
            "placeholder": "​",
            "style": "IPY_MODEL_e3cf01ab01624f23af90c0a40dc98bfc",
            "value": " 5/5 [01:29&lt;00:00, 15.43s/it]"
          }
        },
        "27c5fc172dc74b509592d3f6ea88355b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3292274ae5a749758efd4081f9b18e50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b218f8b9879f4c87b03d1e23fbdfb5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3472915729914fda80ce2a5632a56f24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "548fc4124532457d867594d57fac9d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7094db87f924830af9bffdbbfaa97fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3cf01ab01624f23af90c0a40dc98bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "655c4a982770480294d792d8a7928e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd8ef7a7b2cb44d6852cbca09936fc3e",
              "IPY_MODEL_9c10bf86c7b046359d9b6c3047706dab",
              "IPY_MODEL_022b5ebb70254bd3bd0a058bbead07ad"
            ],
            "layout": "IPY_MODEL_16e7be46f983419b819f34334195e1d9"
          }
        },
        "cd8ef7a7b2cb44d6852cbca09936fc3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_868f056941614201b18503611bb0a035",
            "placeholder": "​",
            "style": "IPY_MODEL_1c1e3bda3bc34267a11cb19e5fda633c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9c10bf86c7b046359d9b6c3047706dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45f1719bb4ef4528bcb1dbf35b766220",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fee040eca054d838485798843dacadc",
            "value": 4
          }
        },
        "022b5ebb70254bd3bd0a058bbead07ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ac40aa28d5c4aafa72b5f6c4a4cd624",
            "placeholder": "​",
            "style": "IPY_MODEL_0702c0ae10b644b285536776244f7e56",
            "value": " 4/4 [01:29&lt;00:00, 19.82s/it]"
          }
        },
        "16e7be46f983419b819f34334195e1d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868f056941614201b18503611bb0a035": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c1e3bda3bc34267a11cb19e5fda633c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45f1719bb4ef4528bcb1dbf35b766220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fee040eca054d838485798843dacadc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ac40aa28d5c4aafa72b5f6c4a4cd624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0702c0ae10b644b285536776244f7e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWpiEwLdSbZM",
        "outputId": "9ec2fd13-a326-4295-9010-02d2eac024ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "T1Fu-aEjSy_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef158b87-11f9-43ec-e524-a48ede75ac42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 16 05:17:16 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning LLama-Factory"
      ],
      "metadata": {
        "id": "pSJLmELfMHFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/sarvam-ai\n",
        "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd /content/drive/MyDrive/sarvam-ai/LLaMA-Factory\n",
        "!pip install -e \".[torch,metrics]\" --no-build-isolation\n"
      ],
      "metadata": {
        "id": "t1LUrMoyVa3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35a2438c-6d52-4bfb-ccc7-8ae7504c6e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/sarvam-ai/LLaMA-Factory\n",
            "Obtaining file:///content/drive/MyDrive/sarvam-ai/LLaMA-Factory\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets<=3.6.0,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: accelerate<=1.7.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.7.0)\n",
            "Requirement already satisfied: peft<=0.15.2,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.15.2)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
            "Requirement already satisfied: gradio<=5.31.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.31.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.15.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.5)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.34.3)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n",
            "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
            "  Downloading sse_starlette-2.3.6-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Collecting fire (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Collecting numpy<2.0.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<=2.10.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Collecting av (from llamafactory==0.9.3.dev0)\n",
            "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.52.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.9.3.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2025.3.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.18)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.2.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.14.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->llamafactory==0.9.3.dev0) (4.9.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.1.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.20.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Downloading sse_starlette-2.3.6-py3-none-any.whl (10 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=27451 sha256=2004ad640484bd40798f8316889153de86fcead587a80b2238d87c199a761559\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rv1_4b58/wheels/a6/c8/b9/395622bcfe1961bfb0993b06fe9452a22c0f67df904f0c0737\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=7b343ad20dcfbe9e513aeffdf285ac79e2784506c89852cba3520ded7694c00b\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: shtab, rouge-chinese, pydantic-core, numpy, fire, av, sse-starlette, pydantic, tyro, trl, llamafactory\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.5\n",
            "    Uninstalling pydantic-2.11.5:\n",
            "      Successfully uninstalled pydantic-2.11.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed av-14.4.0 fire-0.7.0 llamafactory-0.9.3.dev0 numpy-1.26.4 pydantic-2.10.6 pydantic-core-2.27.2 rouge-chinese-1.0.3 shtab-1.7.2 sse-starlette-2.3.6 trl-0.9.6 tyro-0.8.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "e6ed00521e2b440aa8c247127bfb9e85"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/sarvam-ai"
      ],
      "metadata": {
        "id": "1kH-FolEVOSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
        "# !pip install datasets accelerate peft trl deepspeed bitsandbytes flash-attn\n",
        "# !pip install -U \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "id": "3VfV-lVgWN-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli download Qwen/Qwen2.5-VL-7B-Instruct --local-dir /content/drive/MyDrive/sarvam-ai/qwen25_7b_instruct"
      ],
      "metadata": {
        "id": "x6FxcJkFZCeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/sarvam-ai/LLaMA-Factory\n",
        "# !llamafactory-cli train /content/drive/MyDrive/sarvam-ai/LLaMA-Factory/examples/train_lora/qwen2_5vl_lora_sft.yaml"
      ],
      "metadata": {
        "id": "unzgV7v9XPHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !llamafactory-cli export /content/drive/MyDrive/sarvam-ai/LLaMA-Factory/examples/merge_lora/qwen2_5vl_lora_sft.yaml"
      ],
      "metadata": {
        "id": "xt9vJXIbbPHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !llamafactory-cli api /content/drive/MyDrive/sarvam-ai/LLaMA-Factory/examples/inference/qwen2_5vl.yaml"
      ],
      "metadata": {
        "id": "vbj2Cfoxdiuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencing"
      ],
      "metadata": {
        "id": "59f4ejHkMPR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio transformers datasets peft trl deepspeed bitsandbytes flash-attn -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toPMJmxI7W87",
        "outputId": "0409cf4c-114e-420e-e50c-a58d34da0668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Found existing installation: transformers 4.52.4\n",
            "Uninstalling transformers-4.52.4:\n",
            "  Successfully uninstalled transformers-4.52.4\n",
            "Found existing installation: datasets 3.6.0\n",
            "Uninstalling datasets-3.6.0:\n",
            "  Successfully uninstalled datasets-3.6.0\n",
            "Found existing installation: peft 0.15.2\n",
            "Uninstalling peft-0.15.2:\n",
            "  Successfully uninstalled peft-0.15.2\n",
            "Found existing installation: trl 0.18.2\n",
            "Uninstalling trl-0.18.2:\n",
            "  Successfully uninstalled trl-0.18.2\n",
            "Found existing installation: deepspeed 0.17.1\n",
            "Uninstalling deepspeed-0.17.1:\n",
            "  Successfully uninstalled deepspeed-0.17.1\n",
            "Found existing installation: bitsandbytes 0.46.0\n",
            "Uninstalling bitsandbytes-0.46.0:\n",
            "  Successfully uninstalled bitsandbytes-0.46.0\n",
            "Found existing installation: flash_attn 2.8.0.post2\n",
            "Uninstalling flash_attn-2.8.0.post2:\n",
            "  Successfully uninstalled flash_attn-2.8.0.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126\n",
        "# !pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
        "# !pip install transformers==4.51.3 accelerate\n",
        "!pip install -U transformers accelerate\n",
        "!pip install datasets peft trl deepspeed bitsandbytes\n",
        "!pip install qwen_vl_utils\n",
        "!pip install packaging ninja\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install triton\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZbhakWSvdX93",
        "outputId": "2d1119cd-3e6c-478e-8745-74c0e876bb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 102\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Collecting torch==2.6.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.21.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio==2.6.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.14.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp311-cp311-manylinux_2_28_x86_64.whl (764.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.6/764.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchvision-0.21.0%2Bcu126-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp311-cp311-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 torch-2.6.0+cu126 torchaudio-2.6.0+cu126 torchvision-0.21.0+cu126\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "functorch",
                  "torch",
                  "torchaudio",
                  "torchgen",
                  "torchvision",
                  "torio"
                ]
              },
              "id": "6dda5426487141b891e73144bc5d1cc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "Successfully installed transformers-4.52.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              },
              "id": "1ef16f45cda14f1d970d1e00b9414884"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.18.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.17.1.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.7.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.11/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.11.1.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.5)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.575.51)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.18.2-py3-none-any.whl (366 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.4/366.4 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.17.1-py3-none-any.whl size=1690724 sha256=90be1503883580aacfb55f1f8a8dadaf02a04618e483c844e30da044f3c168fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/86/36/22db26525829160fd1c4add33d8a834ec046b90abf45cd363b\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: deepspeed, datasets, bitsandbytes, trl, peft\n",
            "Successfully installed bitsandbytes-0.46.0 datasets-3.6.0 deepspeed-0.17.1 peft-0.15.2 trl-0.18.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "bitsandbytes",
                  "datasets",
                  "deepspeed",
                  "trl"
                ]
              },
              "id": "4aaede97927a4760a545bd9d209e04b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qwen_vl_utils in /usr/local/lib/python3.11/dist-packages (0.0.11)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (14.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (24.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (11.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen_vl_utils) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen_vl_utils) (2025.4.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (24.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (1.11.1.4)\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.0.post2.tar.gz (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.14.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.0.post2-cp311-cp311-linux_x86_64.whl size=255936973 sha256=a2566ec0cc6cf2330252030f8bf41f87fea11503a211e1f76b552c038d38af77\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/75/55/57ba1e272fd7fa1a01d9ba6b5334b7adaabf79900ede22c040\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.0.post2\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "libraries = [\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"torchaudio\",\n",
        "    \"datasets\",\n",
        "    \"transformers\",\n",
        "    \"accelerate\",\n",
        "    \"peft\",\n",
        "    \"trl\",\n",
        "    \"deepspeed\",\n",
        "    \"bitsandbytes\",\n",
        "    \"flash_attn\",\n",
        "    'triton'\n",
        "]\n",
        "\n",
        "print(\"📦 Installed Library Versions:\\n\")\n",
        "\n",
        "for lib in libraries:\n",
        "    try:\n",
        "        module = importlib.import_module(lib)\n",
        "        version = getattr(module, \"__version__\", \"Version info not available\")\n",
        "        print(f\"{lib:<15} : {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"{lib:<15} : ❌ Not Installed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF-4Xnts31lC",
        "outputId": "d4ddcc18-38e6-40c9-f0f8-9dc92de9b1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installed Library Versions:\n",
            "\n",
            "torch           : 2.6.0+cu126\n",
            "torchvision     : 0.21.0+cu126\n",
            "torchaudio      : 2.6.0+cu126\n",
            "datasets        : 3.6.0\n",
            "transformers    : 4.52.4\n",
            "accelerate      : 1.7.0\n",
            "peft            : 0.15.2\n",
            "trl             : 0.18.2\n",
            "[2025-06-16 05:29:29,535] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-06-16 05:29:31,317] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
            "deepspeed       : 0.17.1\n",
            "bitsandbytes    : 0.46.0\n",
            "flash_attn      : 2.8.0.post2\n",
            "triton          : 3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMFXNfw-dBlY",
        "outputId": "38ebeeea-aa28-4127-c2e0-c12600340305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVW5suQ9uq-q",
        "outputId": "389536e1-3b31-4b03-991e-4247b630479f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ------ BASE MODEL INFERENCE ON ENHANCED IMAGES ------------\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import time\n",
        "# === Config ===\n",
        "image_folder = \"/content/drive/MyDrive/sarvam-ai/dataset/enhanced_image_test\"\n",
        "output_folder = \"/content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results\"\n",
        "model_path = \"/content/drive/MyDrive/sarvam-ai/qwen25_7b_instruct\"\n",
        "prompt_text = \"Extract all the plain text from the image, both Gujarati and English keeping the structure and format consistent.\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# === Load model and processor once ===\n",
        "print(\"🔧 Loading model and processor...\")\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "# === Inference loop ===\n",
        "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "for image_name in image_files:\n",
        "    t1 = time.time()\n",
        "    try:\n",
        "        print(f\"\\n🖼️ Processing image: {image_name}\")\n",
        "        image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Compose multimodal message\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": prompt_text},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Generate input text\n",
        "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Prepare vision inputs\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "        # Prepare inputs for the model\n",
        "        inputs = processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=False,\n",
        "                temperature=1.0,\n",
        "                max_new_tokens=2048,\n",
        "                repetition_penalty=1.1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Trim and decode\n",
        "        generated_ids_trimmed = [\n",
        "            output[len(input_ids):]\n",
        "            for input_ids, output in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        output_text = tokenizer.batch_decode(\n",
        "            generated_ids_trimmed,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "\n",
        "        # Save output\n",
        "        output_file = os.path.join(output_folder, os.path.splitext(image_name)[0] + \".txt\")\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(output_text)\n",
        "\n",
        "        print(f\"✅ Saved output to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {image_name}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Free image & tensor memory after each iteration\n",
        "        del image, messages, image_inputs, video_inputs, inputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    t2 = time.time()\n",
        "    print(f\"Time taken per image: {t2-t1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cf40e291507c4803a83505afb67c18f4",
            "ac77aa0c58594fc398756c394702c549",
            "5b57f2a739ef4ec2a9165f1f07c8c835",
            "57e419b77f174a87a90092827d3658cf",
            "0aff0eae6a584d6e8dc867e305b3b9df",
            "b6603dd5266946ea9b74f3d9ece5c9ff",
            "3bc01c933fd64de0870577cbff1c74d2",
            "59f4f5d7523545569f1ec2c48554b35f",
            "7c77af80aec846aea5b9561411726c62",
            "5e55ece055b64565ba6aba7c7b5ced52",
            "25f949a2ec4d4e4fa47d606d320d1a27"
          ]
        },
        "id": "eudKF7i9dzc_",
        "outputId": "27fff4b4-6bda-4658-8fe5-cd3e12a1b691"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Loading model and processor...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf40e291507c4803a83505afb67c18f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0081_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0081_binarization.txt\n",
            "Time taken per image: 94.32020735740662\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0082_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0082_binarization.txt\n",
            "Time taken per image: 91.40675759315491\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0083_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0083_binarization.txt\n",
            "Time taken per image: 91.17771339416504\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0084_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0084_binarization.txt\n",
            "Time taken per image: 91.42752814292908\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0085_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0085_binarization.txt\n",
            "Time taken per image: 91.43370032310486\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0086_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0086_binarization.txt\n",
            "Time taken per image: 91.84735941886902\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0087_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0087_binarization.txt\n",
            "Time taken per image: 91.7156708240509\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0088_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0088_binarization.txt\n",
            "Time taken per image: 91.12763285636902\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0089_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0089_binarization.txt\n",
            "Time taken per image: 91.69722652435303\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0090_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0090_binarization.txt\n",
            "Time taken per image: 81.5938332080841\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0091_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0091_binarization.txt\n",
            "Time taken per image: 90.86175203323364\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0092_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0092_binarization.txt\n",
            "Time taken per image: 91.50666689872742\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0093_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0093_binarization.txt\n",
            "Time taken per image: 91.39030075073242\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0094_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0094_binarization.txt\n",
            "Time taken per image: 91.12442803382874\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0095_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0095_binarization.txt\n",
            "Time taken per image: 91.76421093940735\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0096_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0096_binarization.txt\n",
            "Time taken per image: 95.10013484954834\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0097_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0097_binarization.txt\n",
            "Time taken per image: 92.13281917572021\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0098_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0098_binarization.txt\n",
            "Time taken per image: 91.91353607177734\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0099_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0099_binarization.txt\n",
            "Time taken per image: 91.19757866859436\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0100_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_enhanced_images_results/Swarupsannidhan_0100_binarization.txt\n",
            "Time taken per image: 91.50526642799377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ------ FINE-TUNED MODEL INFERENCE ON ENHANCED IMAGES ------------\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import time\n",
        "# === Config ===\n",
        "image_folder = \"/content/drive/MyDrive/sarvam-ai/dataset/enhanced_image_test\"\n",
        "output_folder = \"/content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results\"\n",
        "model_path = \"/content/drive/MyDrive/sarvam-ai/LLaMA-Factory/output/qwen2_5vl_7b_lora_merged_sarvam\"\n",
        "prompt_text = \"Extract all the plain text from the image, both Gujarati and English keeping the structure and format consistent.\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# === Load model and processor once ===\n",
        "print(\"🔧 Loading model and processor...\")\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "# === Inference loop ===\n",
        "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "for image_name in image_files:\n",
        "    t1 = time.time()\n",
        "    try:\n",
        "        print(f\"\\n🖼️ Processing image: {image_name}\")\n",
        "        image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Compose multimodal message\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": prompt_text},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Generate input text\n",
        "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Prepare vision inputs\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "        # Prepare inputs for the model\n",
        "        inputs = processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=False,\n",
        "                temperature=1.0,\n",
        "                max_new_tokens=2048,\n",
        "                repetition_penalty=1.1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Trim and decode\n",
        "        generated_ids_trimmed = [\n",
        "            output[len(input_ids):]\n",
        "            for input_ids, output in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        output_text = tokenizer.batch_decode(\n",
        "            generated_ids_trimmed,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "\n",
        "        # Save output\n",
        "        output_file = os.path.join(output_folder, os.path.splitext(image_name)[0] + \".txt\")\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(output_text)\n",
        "\n",
        "        print(f\"✅ Saved output to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {image_name}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Free image & tensor memory after each iteration\n",
        "        del image, messages, image_inputs, video_inputs, inputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    t2 = time.time()\n",
        "    print(f\"Time taken per image: {t2-t1}\")\n"
      ],
      "metadata": {
        "id": "vOuiQJscGSBK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c446a8af867a4bab86942978f3b67408",
            "a661da9178774e4e927e83c2b3a1e745",
            "2cd565ce055144be8adf9d33cfccb2da",
            "fcffc5c564a74a9a9f576815528d0ec2",
            "f7d7b628b9864cdf97a51f6e36846987",
            "10f71b2d93594be0854d032156a67a8d",
            "3c70f31ce71d4176ad4af2f5a29063d7",
            "44941ed6848948da8949b989cd1abf2a",
            "ce4a41da44054e4aabac58d611e70380",
            "51d8cad0288d4b51b02a99b94dc1829e",
            "7c68506d6e4f4de99f8d80f6377d86fa"
          ]
        },
        "outputId": "4b157ed5-d1e5-414a-c09b-c472c3a3c887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Loading model and processor...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c446a8af867a4bab86942978f3b67408"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0081_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0081_binarization.txt\n",
            "Time taken per image: 94.69000363349915\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0082_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0082_binarization.txt\n",
            "Time taken per image: 90.06886625289917\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0083_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0083_binarization.txt\n",
            "Time taken per image: 89.93379831314087\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0084_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0084_binarization.txt\n",
            "Time taken per image: 90.00136637687683\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0085_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0085_binarization.txt\n",
            "Time taken per image: 90.35422897338867\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0086_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0086_binarization.txt\n",
            "Time taken per image: 90.18896579742432\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0087_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0087_binarization.txt\n",
            "Time taken per image: 89.99178838729858\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0088_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0088_binarization.txt\n",
            "Time taken per image: 90.03748941421509\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0089_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0089_binarization.txt\n",
            "Time taken per image: 90.22377848625183\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0090_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0090_binarization.txt\n",
            "Time taken per image: 82.76490664482117\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0091_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0091_binarization.txt\n",
            "Time taken per image: 90.05807757377625\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0092_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0092_binarization.txt\n",
            "Time taken per image: 90.36174869537354\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0093_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0093_binarization.txt\n",
            "Time taken per image: 36.97890591621399\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0094_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0094_binarization.txt\n",
            "Time taken per image: 90.37180733680725\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0095_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0095_binarization.txt\n",
            "Time taken per image: 90.06081032752991\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0096_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0096_binarization.txt\n",
            "Time taken per image: 89.9887330532074\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0097_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0097_binarization.txt\n",
            "Time taken per image: 90.12405943870544\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0098_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0098_binarization.txt\n",
            "Time taken per image: 90.30645561218262\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0099_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0099_binarization.txt\n",
            "Time taken per image: 90.08019685745239\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0100_binarization.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_enhanced_images_results/Swarupsannidhan_0100_binarization.txt\n",
            "Time taken per image: 90.21721935272217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ------ BASE MODEL INFERENCE ON ORIGINAL IMAGES ------------\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import time\n",
        "# === Config ===\n",
        "image_folder = \"/content/drive/MyDrive/sarvam-ai/dataset/original_test\"\n",
        "output_folder = \"/content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results\"\n",
        "model_path = \"/content/drive/MyDrive/sarvam-ai/qwen25_7b_instruct\"\n",
        "prompt_text = \"Extract all the plain text from the image, both Gujarati and English keeping the structure and format consistent.\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# === Load model and processor once ===\n",
        "print(\"🔧 Loading model and processor...\")\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "# === Inference loop ===\n",
        "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "for image_name in image_files:\n",
        "    t1 = time.time()\n",
        "    try:\n",
        "        print(f\"\\n🖼️ Processing image: {image_name}\")\n",
        "        image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Compose multimodal message\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": prompt_text},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Generate input text\n",
        "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Prepare vision inputs\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "        # Prepare inputs for the model\n",
        "        inputs = processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=False,\n",
        "                temperature=1.0,\n",
        "                max_new_tokens=2048,\n",
        "                repetition_penalty=1.1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Trim and decode\n",
        "        generated_ids_trimmed = [\n",
        "            output[len(input_ids):]\n",
        "            for input_ids, output in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        output_text = tokenizer.batch_decode(\n",
        "            generated_ids_trimmed,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "\n",
        "        # Save output\n",
        "        output_file = os.path.join(output_folder, os.path.splitext(image_name)[0] + \".txt\")\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(output_text)\n",
        "\n",
        "        print(f\"✅ Saved output to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {image_name}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Free image & tensor memory after each iteration\n",
        "        del image, messages, image_inputs, video_inputs, inputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    t2 = time.time()\n",
        "    print(f\"Time taken per image: {t2-t1}\")\n"
      ],
      "metadata": {
        "id": "bYP7YE5V3hIz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7c18f5b9fc744767a6a6fe754f47254c",
            "9ff58536d5d140a78543e550ab2338fe",
            "614e4844b4f2403bb7db01ce41dc7e47",
            "78c7658e7f514275ad861272c69aebbe",
            "27c5fc172dc74b509592d3f6ea88355b",
            "3292274ae5a749758efd4081f9b18e50",
            "b218f8b9879f4c87b03d1e23fbdfb5d7",
            "3472915729914fda80ce2a5632a56f24",
            "548fc4124532457d867594d57fac9d4a",
            "b7094db87f924830af9bffdbbfaa97fd",
            "e3cf01ab01624f23af90c0a40dc98bfc"
          ]
        },
        "outputId": "0ebc1f2a-ebcf-4ede-ac5a-be60fa3deb9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Loading model and processor...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c18f5b9fc744767a6a6fe754f47254c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0088.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0088.txt\n",
            "Time taken per image: 93.60796856880188\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0085.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0085.txt\n",
            "Time taken per image: 91.7735345363617\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0090.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0090.txt\n",
            "Time taken per image: 82.0425636768341\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0091.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0091.txt\n",
            "Time taken per image: 91.8741762638092\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0092.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0092.txt\n",
            "Time taken per image: 91.60574340820312\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0087.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0087.txt\n",
            "Time taken per image: 75.6070122718811\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0086.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0086.txt\n",
            "Time taken per image: 91.69553565979004\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0082.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0082.txt\n",
            "Time taken per image: 91.68864703178406\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0089.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0089.txt\n",
            "Time taken per image: 91.77088594436646\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0099.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0099.txt\n",
            "Time taken per image: 91.94874382019043\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0094.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0094.txt\n",
            "Time taken per image: 72.40954422950745\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0083.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0083.txt\n",
            "Time taken per image: 92.22019481658936\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0081.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0081.txt\n",
            "Time taken per image: 71.3754632472992\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0098.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0098.txt\n",
            "Time taken per image: 91.86000442504883\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0096.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0096.txt\n",
            "Time taken per image: 91.5773229598999\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0097.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0097.txt\n",
            "Time taken per image: 91.8384358882904\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0095.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0095.txt\n",
            "Time taken per image: 92.2968680858612\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0100.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0100.txt\n",
            "Time taken per image: 91.85633254051208\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0084.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0084.txt\n",
            "Time taken per image: 91.83378958702087\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0093.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_base_original_images_results/Swarupsannidhan_0093.txt\n",
            "Time taken per image: 91.59921336174011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ------ FINE-TUNED MODEL INFERENCE ON ORIGINAL IMAGES ------------\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from PIL import Image\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import time\n",
        "# === Config ===\n",
        "image_folder = \"/content/drive/MyDrive/sarvam-ai/dataset/original_test\"\n",
        "output_folder = \"/content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results\"\n",
        "model_path = \"/content/drive/MyDrive/sarvam-ai/LLaMA-Factory/output/qwen2_5vl_7b_lora_merged_sarvam\"\n",
        "prompt_text = \"Extract all the plain text from the image, both Gujarati and English keeping the structure and format consistent.\"\n",
        "\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# === Load model and processor once ===\n",
        "print(\"🔧 Loading model and processor...\")\n",
        "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "# === Inference loop ===\n",
        "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "for image_name in image_files:\n",
        "    t1 = time.time()\n",
        "    try:\n",
        "        print(f\"\\n🖼️ Processing image: {image_name}\")\n",
        "        image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Compose multimodal message\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": prompt_text},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Generate input text\n",
        "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Prepare vision inputs\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "        # Prepare inputs for the model\n",
        "        inputs = processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                do_sample=False,\n",
        "                temperature=1.0,\n",
        "                max_new_tokens=2048,\n",
        "                repetition_penalty=1.1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Trim and decode\n",
        "        generated_ids_trimmed = [\n",
        "            output[len(input_ids):]\n",
        "            for input_ids, output in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        output_text = tokenizer.batch_decode(\n",
        "            generated_ids_trimmed,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "\n",
        "        # Save output\n",
        "        output_file = os.path.join(output_folder, os.path.splitext(image_name)[0] + \".txt\")\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(output_text)\n",
        "\n",
        "        print(f\"✅ Saved output to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {image_name}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Free image & tensor memory after each iteration\n",
        "        del image, messages, image_inputs, video_inputs, inputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    t2 = time.time()\n",
        "    print(f\"Time taken per image: {t2-t1}\")\n"
      ],
      "metadata": {
        "id": "vDZPBJYmJwCk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "655c4a982770480294d792d8a7928e5e",
            "cd8ef7a7b2cb44d6852cbca09936fc3e",
            "9c10bf86c7b046359d9b6c3047706dab",
            "022b5ebb70254bd3bd0a058bbead07ad",
            "16e7be46f983419b819f34334195e1d9",
            "868f056941614201b18503611bb0a035",
            "1c1e3bda3bc34267a11cb19e5fda633c",
            "45f1719bb4ef4528bcb1dbf35b766220",
            "8fee040eca054d838485798843dacadc",
            "1ac40aa28d5c4aafa72b5f6c4a4cd624",
            "0702c0ae10b644b285536776244f7e56"
          ]
        },
        "outputId": "40cc9c44-9509-4987-8b59-21232ee55a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Loading model and processor...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "655c4a982770480294d792d8a7928e5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0088.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0088.txt\n",
            "Time taken per image: 93.80875706672668\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0085.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0085.txt\n",
            "Time taken per image: 91.97397041320801\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0090.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0090.txt\n",
            "Time taken per image: 81.62248015403748\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0091.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0091.txt\n",
            "Time taken per image: 90.42598009109497\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0092.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0092.txt\n",
            "Time taken per image: 92.71603059768677\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0087.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0087.txt\n",
            "Time taken per image: 92.18194055557251\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0086.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0086.txt\n",
            "Time taken per image: 92.17513489723206\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0082.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0082.txt\n",
            "Time taken per image: 92.1903829574585\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0089.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0089.txt\n",
            "Time taken per image: 92.02966856956482\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0099.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0099.txt\n",
            "Time taken per image: 91.75163650512695\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0094.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0094.txt\n",
            "Time taken per image: 91.86941695213318\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0083.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0083.txt\n",
            "Time taken per image: 92.1437304019928\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0081.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0081.txt\n",
            "Time taken per image: 91.83161187171936\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0098.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0098.txt\n",
            "Time taken per image: 91.76487398147583\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0096.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0096.txt\n",
            "Time taken per image: 91.6953113079071\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0097.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0097.txt\n",
            "Time taken per image: 92.23330068588257\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0095.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0095.txt\n",
            "Time taken per image: 92.86357069015503\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0100.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0100.txt\n",
            "Time taken per image: 92.43209648132324\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0084.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0084.txt\n",
            "Time taken per image: 91.93801259994507\n",
            "\n",
            "🖼️ Processing image: Swarupsannidhan_0093.png\n",
            "✅ Saved output to /content/drive/MyDrive/sarvam-ai/qwen25_vl_lora_finetune_original_images_results/Swarupsannidhan_0093.txt\n",
            "Time taken per image: 91.69658493995667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization using vLLM"
      ],
      "metadata": {
        "id": "fddomo-1MTK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AWQ"
      ],
      "metadata": {
        "id": "3o1EVDlwMYra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install autoawq"
      ],
      "metadata": {
        "id": "qq-_lTHvMWyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_path = '/content/drive/MyDrive/sarvam-ai/LLaMA-Factory/output/qwen2_5vl_7b_lora_merged_sarvam'\n",
        "quant_path = '/content/drive/MyDrive/sarvam-ai/qwen2_5vl_7b_lora_awq'\n",
        "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
        "\n",
        "# Load model\n",
        "model = AutoAWQForCausalLM.from_pretrained(\n",
        "    model_path, low_cpu_mem_usage=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# Quantize\n",
        "model.quantize(tokenizer, quant_config=quant_config)\n",
        "\n",
        "# Save quantized model\n",
        "model.save_quantized(quant_path)\n",
        "tokenizer.save_pretrained(quant_path)\n",
        "\n",
        "print(f'Model is quantized and saved at \"{quant_path}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "6203193365264626b3fed486ea0d9a37",
            "e5396b20045541f4bfe1b2b230f34131",
            "56b64c4c95794fdc84f536f79bb864f9",
            "23d42eb3cd5d43c79815de655c3e1ef4",
            "4a37d9960b2d44a787d38a7972355830",
            "8e34262023d447fab5247133d8a2aa7a",
            "d563827595f44e958850d4bd58154b19",
            "d3440bc765014c8eb6755c6f3f923d67",
            "a7c77deeb2a44f0eb1c39e4aa56ceb3f",
            "61860e794f074a3abff4c0df13c5d789",
            "3a0153f03b654436b4cb9ef2f0c3d942"
          ]
        },
        "id": "dNbxmrUWPCW5",
        "outputId": "1266bd17-8fa7-4dbf-f6cc-7ba6bb05a662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6203193365264626b3fed486ea0d9a37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Qwen2_5_VLModel' object has no attribute 'layers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-355801832>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Quantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Save quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/awq/models/base.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(self, tokenizer, quant_config, calib_data, split, text_column, duo_scaling, export_compatible, apply_clip, n_parallel_calib_samples, max_calib_samples, max_calib_seq_len, max_chunk_memory, quantizer_cls, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules_to_not_convert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules_to_not_convert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         self.quantizer = quantizer_cls(\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/awq/quantize/quantizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, awq_model, model, tokenizer, w_bit, group_size, zero_point, version, calib_data, split, text_column, duo_scaling, modules_to_not_convert, export_compatible, apply_clip, n_parallel_calib_samples, max_calib_samples, max_calib_seq_len, max_chunk_memory)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mmodules_to_not_convert\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodules_to_not_convert\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 70\u001b[0;31m         self.modules, self.module_kwargs, self.inps = self.init_quant(\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_calib_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_calib_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/awq/quantize/quantizer.py\u001b[0m in \u001b[0;36minit_quant\u001b[0;34m(self, n_samples, max_seq_len)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         samples = get_calib_dataset(\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalib_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/awq/models/qwen2_5_vl.py\u001b[0m in \u001b[0;36mget_model_layers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_model_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Qwen2_5_VLForConditionalGeneration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Qwen2_5_VLModel' object has no attribute 'layers'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from awq import AutoAWQForCausalLM\n",
        "from awq.utils.qwen_vl_utils import process_vision_info\n",
        "from awq.quantize.quantizer import AwqQuantizer, clear_memory, get_best_device\n",
        "\n",
        "# Specify paths and hyperparameters for quantization\n",
        "model_path = '/content/drive/MyDrive/sarvam-ai/LLaMA-Factory/output/qwen2_5vl_7b_lora_merged_sarvam'\n",
        "quant_path = '/content/drive/MyDrive/sarvam-ai/qwen2_5vl_7b_lora_awq'\n",
        "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
        "\n",
        "model = AutoAWQForCausalLM.from_pretrained(\n",
        "    model_path, attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "\n",
        "# We define our own quantizer by extending the AwqQuantizer.\n",
        "# The main difference is in how the samples are processed when\n",
        "# the quantization process initialized.\n",
        "class Qwen2VLAwqQuantizer(AwqQuantizer):\n",
        "    def init_quant(self, n_samples=None, max_seq_len=None):\n",
        "        modules = self.awq_model.get_model_layers(self.model)\n",
        "        samples = self.calib_data\n",
        "\n",
        "        inps = []\n",
        "        layer_kwargs = {}\n",
        "\n",
        "        best_device = get_best_device()\n",
        "        modules[0] = modules[0].to(best_device)\n",
        "        self.awq_model.move_embed(self.model, best_device)\n",
        "\n",
        "        # get input and kwargs to layer 0\n",
        "        # with_kwargs is only supported in PyTorch 2.0\n",
        "        # use this Catcher hack for now\n",
        "        class Catcher(nn.Module):\n",
        "            def __init__(self, module):\n",
        "                super().__init__()\n",
        "                self.module = module\n",
        "\n",
        "            def forward(self, *args, **kwargs):\n",
        "                # assume first input to forward is hidden states\n",
        "                if len(args) > 0:\n",
        "                    hidden_states = args[0]\n",
        "                    del args\n",
        "                else:\n",
        "                    first_key = list(kwargs.keys())[0]\n",
        "                    hidden_states = kwargs.pop(first_key)\n",
        "\n",
        "                inps.append(hidden_states)\n",
        "                layer_kwargs.update(kwargs)\n",
        "                raise ValueError  # early exit to break later inference\n",
        "\n",
        "        def move_to_device(obj: torch.Tensor | nn.Module, device: torch.device):\n",
        "            def get_device(obj: torch.Tensor | nn.Module):\n",
        "                if isinstance(obj, torch.Tensor):\n",
        "                    return obj.device\n",
        "                return next(obj.parameters()).device\n",
        "\n",
        "            if get_device(obj) != device:\n",
        "                obj = obj.to(device)\n",
        "            return obj\n",
        "\n",
        "        # patch layer 0 to catch input and kwargs\n",
        "        modules[0] = Catcher(modules[0])\n",
        "        for k, v in samples.items():\n",
        "            if isinstance(v, (torch.Tensor, nn.Module)):\n",
        "                samples[k] = move_to_device(v, best_device)\n",
        "        try:\n",
        "            self.model(**samples)\n",
        "        except ValueError:  # work with early exit\n",
        "            pass\n",
        "        finally:\n",
        "            for k, v in samples.items():\n",
        "                if isinstance(v, (torch.Tensor, nn.Module)):\n",
        "                    samples[k] = move_to_device(v, \"cpu\")\n",
        "        modules[0] = modules[0].module  # restore\n",
        "\n",
        "        del samples\n",
        "        inps = inps[0]\n",
        "\n",
        "        modules[0] = modules[0].cpu()\n",
        "        self.awq_model.move_embed(self.model, \"cpu\")\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "        return modules, layer_kwargs, inps\n",
        "\n",
        "# Then you need to prepare your data for calibaration. What you need to do is just put samples into a list,\n",
        "# each of which is a typical chat message as shown below. you can specify text and image in `content` field:\n",
        "# dataset = [\n",
        "#     # message 0\n",
        "#     [\n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "#         {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\n",
        "#         {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"},\n",
        "#     ],\n",
        "#     # message 1\n",
        "#     [\n",
        "#         {\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": [\n",
        "#                 {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n",
        "#                 {\"type\": \"text\", \"text\": \"Output all text in the image\"},\n",
        "#             ],\n",
        "#         },\n",
        "#         {\"role\": \"assistant\", \"content\": \"The text in the image is balabala...\"},\n",
        "#     ],\n",
        "#     # other messages...\n",
        "#     ...,\n",
        "# ]\n",
        "# here, we use a caption dataset **only for demonstration**. You should replace it with your own sft dataset.\n",
        "def prepare_dataset(n_sample: int = 8) -> list[list[dict]]:\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    dataset = load_dataset(\"laion/220k-GPT4Vision-captions-from-LIVIS\", split=f\"train[:{n_sample}]\")\n",
        "    return [\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": sample[\"url\"]},\n",
        "                    {\"type\": \"text\", \"text\": \"generate a caption for this image\"},\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"assistant\", \"content\": sample[\"caption\"]},\n",
        "        ]\n",
        "        for sample in dataset\n",
        "    ]\n",
        "\n",
        "dataset = prepare_dataset()\n",
        "\n",
        "# process the dataset into tensors\n",
        "text = model.processor.apply_chat_template(dataset, tokenize=False, add_generation_prompt=True)\n",
        "image_inputs, video_inputs = process_vision_info(dataset)\n",
        "inputs = model.processor(text=text, images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Then just run the calibration process by one line of code\n",
        "model.quantize(calib_data=inputs, quant_config=quant_config, quantizer_cls=Qwen2VLAwqQuantizer)\n",
        "\n",
        "# Save the model\n",
        "model.model.config.use_cache = model.model.generation_config.use_cache = True\n",
        "model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")"
      ],
      "metadata": {
        "id": "KaN1GXl1Upex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## https://casper-hansen.github.io/AutoAWQ/examples/#vision-language-models"
      ],
      "metadata": {
        "id": "17IGI81KNUUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPTQ"
      ],
      "metadata": {
        "id": "qrWl4wMKViOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip cache purge\n",
        "# !pip uninstall gptqmodel\n",
        "\n",
        "## Takes 15 min to build -> 4.0.0-dev\n",
        "%cd /content/drive/MyDrive/sarvam-ai\n",
        "!git clone https://github.com/ModelCloud/GPTQModel.git\n",
        "%cd GPTQModel\n",
        "!pip install -v . --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzfS0GZvVjY5",
        "outputId": "bc48ad2d-95ce-4e7d-9285-4f0d8a3efca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/sarvam-ai\n",
            "Cloning into 'GPTQModel'...\n",
            "remote: Enumerating objects: 18431, done.\u001b[K\n",
            "remote: Counting objects: 100% (1402/1402), done.\u001b[K\n",
            "remote: Compressing objects: 100% (656/656), done.\u001b[K\n",
            "remote: Total 18431 (delta 1199), reused 746 (delta 746), pack-reused 17029 (from 3)\u001b[K\n",
            "Receiving objects: 100% (18431/18431), 12.52 MiB | 16.87 MiB/s, done.\n",
            "Resolving deltas: 100% (13518/13518), done.\n",
            "Updating files: 100% (841/841), done.\n",
            "/content/drive/MyDrive/sarvam-ai/GPTQModel\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Processing /content/drive/MyDrive/sarvam-ai/GPTQModel\n",
            "  Running command python setup.py egg_info\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/dist.py:333: InformationOnly: Normalizing '4.0.0-dev' to '4.0.0.dev0'\n",
            "    self.metadata.version = self._normalize_version(self.metadata.version)\n",
            "  conda_cuda_include_dir /usr/lib/python3.11/site-packages/nvidia/cuda_runtime/include\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-ttxlxhtq/gptqmodel.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-ttxlxhtq/gptqmodel.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-ttxlxhtq/gptqmodel.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-ttxlxhtq/gptqmodel.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-ttxlxhtq/gptqmodel.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-ttxlxhtq/gptqmodel.egg-info/SOURCES.txt'\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:529: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-ttxlxhtq/gptqmodel.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  no previously-included directories found matching 'format'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-ttxlxhtq/gptqmodel.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (1.7.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.5.3 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: transformers>=4.51.2 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (4.52.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (24.2)\n",
            "Requirement already satisfied: device-smi==0.4.1 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (0.4.1)\n",
            "Requirement already satisfied: protobuf>=5.29.3 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (5.29.5)\n",
            "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (11.2.1)\n",
            "Requirement already satisfied: hf_transfer>=0.1.9 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (0.1.9)\n",
            "Requirement already satisfied: huggingface_hub>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (0.32.4)\n",
            "Requirement already satisfied: random_word==1.0.13 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (1.0.13)\n",
            "Requirement already satisfied: tokenicer==0.0.4 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (0.0.4)\n",
            "Requirement already satisfied: logbar==0.0.4 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (0.0.4)\n",
            "Requirement already satisfied: soundfile==0.13.1 in /usr/local/lib/python3.11/dist-packages (from gptqmodel==4.0.0.dev0) (0.13.1)\n",
            "Requirement already satisfied: autopep8<3.0.0,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from random_word==1.0.13->gptqmodel==4.0.0.dev0) (2.3.2)\n",
            "Requirement already satisfied: pytest<9.0.0,>=8.3.3 in /usr/local/lib/python3.11/dist-packages (from random_word==1.0.13->gptqmodel==4.0.0.dev0) (8.3.5)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from random_word==1.0.13->gptqmodel==4.0.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from random_word==1.0.13->gptqmodel==4.0.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile==0.13.1->gptqmodel==4.0.0.dev0) (1.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.6.0->gptqmodel==4.0.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.30.1->gptqmodel==4.0.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.30.1->gptqmodel==4.0.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.30.1->gptqmodel==4.0.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.30.1->gptqmodel==4.0.0.dev0) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.30.1->gptqmodel==4.0.0.dev0) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.1->gptqmodel==4.0.0.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.1->gptqmodel==4.0.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.2->gptqmodel==4.0.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.2->gptqmodel==4.0.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: pycodestyle>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from autopep8<3.0.0,>=2.3.1->random_word==1.0.13->gptqmodel==4.0.0.dev0) (2.13.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile==0.13.1->gptqmodel==4.0.0.dev0) (2.22)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9.0.0,>=8.3.3->random_word==1.0.13->gptqmodel==4.0.0.dev0) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9.0.0,>=8.3.3->random_word==1.0.13->gptqmodel==4.0.0.dev0) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random_word==1.0.13->gptqmodel==4.0.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random_word==1.0.13->gptqmodel==4.0.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random_word==1.0.13->gptqmodel==4.0.0.dev0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->random_word==1.0.13->gptqmodel==4.0.0.dev0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.1->gptqmodel==4.0.0.dev0) (3.0.2)\n",
            "Building wheels for collected packages: gptqmodel\n",
            "  Running command python setup.py bdist_wheel\n",
            "  conda_cuda_include_dir /usr/lib/python3.11/site-packages/nvidia/cuda_runtime/include\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/dist.py:333: InformationOnly: Normalizing '4.0.0-dev' to '4.0.0.dev0'\n",
            "    self.metadata.version = self._normalize_version(self.metadata.version)\n",
            "  running bdist_wheel\n",
            "  Guessing wheel URL: https://github.com/ModelCloud/GPTQModel/releases/download/v4.0.0-dev/gptqmodel-4.0.0-dev+cu125torch2.6-cp311-cp311-linux_x86_64.whl\n",
            "  wheel name=gptqmodel-4.0.0-dev+cu125torch2.6-cp311-cp311-linux_x86_64.whl\n",
            "  Precompiled wheel not found in url: https://github.com/ModelCloud/GPTQModel/releases/download/v4.0.0-dev/gptqmodel-4.0.0-dev+cu125torch2.6-cp311-cp311-linux_x86_64.whl. Building from source...\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:529: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel\n",
            "  copying gptqmodel/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel\n",
            "  copying gptqmodel/version.py -> build/lib.linux-x86_64-cpython-311/gptqmodel\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/adapter\n",
            "  copying gptqmodel/adapter/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/adapter\n",
            "  copying gptqmodel/adapter/adapter.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/adapter\n",
            "  copying gptqmodel/adapter/peft.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/adapter\n",
            "  copying gptqmodel/adapter/remote.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/adapter\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/eora\n",
            "  copying gptqmodel/eora/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/eora\n",
            "  copying gptqmodel/eora/eora.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/eora\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/dequantize_processor.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/eora_processor.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/gptq_processor.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/input_cache.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/loop_processor.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/module_looper.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/named_module.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/native_processor.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  copying gptqmodel/looper/qqq_processor.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/looper\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/models\n",
            "  copying gptqmodel/models/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models\n",
            "  copying gptqmodel/models/_const.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models\n",
            "  copying gptqmodel/models/auto.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models\n",
            "  copying gptqmodel/models/base.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models\n",
            "  copying gptqmodel/models/loader.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models\n",
            "  copying gptqmodel/models/writer.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules\n",
            "  copying gptqmodel/nn_modules/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules\n",
            "  copying gptqmodel/nn_modules/hooked_linear.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/quantization\n",
            "  copying gptqmodel/quantization/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization\n",
            "  copying gptqmodel/quantization/config.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization\n",
            "  copying gptqmodel/quantization/gptq.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization\n",
            "  copying gptqmodel/quantization/gptqv2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization\n",
            "  copying gptqmodel/quantization/qqq.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization\n",
            "  copying gptqmodel/quantization/quantizer.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/backend.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/bitblas.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/calibration.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/data.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/device.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/eval.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/evalplus.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/exllama.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/hf.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/image.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/importer.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/logger.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/marlin.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/mlx.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/mmlupro.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/model.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/openai_server.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/perplexity.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/plotly.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/python.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/rocm.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/safetensor.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/sglang.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/tensor.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/terminal.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/torch.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/vllm.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  copying gptqmodel/utils/vram.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/utils\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/baichuan.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/base_qwen2_5_omni.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/base_qwen2_vl.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/bloom.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/chatglm.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/codegen.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/cohere.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/cohere2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/dbrx.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/dbrx_converted.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/decilm.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/deepseek_v2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/deepseek_v3.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/dream.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/exaone.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/falcon_h1.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/gemma.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/gemma2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/gemma3.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/glm.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/gpt2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/gpt_bigcode.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/gpt_neox.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/gptj.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/granite.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/grinmoe.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/hymba.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/instella.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/internlm.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/internlm2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/llama.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/longllama.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/mimo.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/minicpm.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/minicpm3.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/mistral.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/mixtral.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/mllama.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/mobilellm.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/moss.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/mpt.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/olmo2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/opt.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/ovis.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/phi.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/phi3.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/phi4.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/qwen.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/qwen2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/qwen2_5_omni.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/qwen2_5_vl.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/qwen2_moe.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/qwen2_vl.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/qwen3.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/qwen3_moe.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/rw.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/stablelmepoch.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/starcoder2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/telechat2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/xverse.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  copying gptqmodel/models/definitions/yi.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/bitblas.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/bitblas_target_detector.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/exllama.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/exllama_eora.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/exllamav2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/ipex.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/marlin.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/qqq.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/torch.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/tritonv2.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  copying gptqmodel/nn_modules/qlinear/utils.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils\n",
            "  copying gptqmodel/nn_modules/triton_utils/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils\n",
            "  copying gptqmodel/nn_modules/triton_utils/custom_autotune.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils\n",
            "  copying gptqmodel/nn_modules/triton_utils/dequant.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils\n",
            "  copying gptqmodel/nn_modules/triton_utils/kernels.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils\n",
            "  copying gptqmodel/nn_modules/triton_utils/mixin.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils\n",
            "  creating build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/rotation\n",
            "  copying gptqmodel/quantization/rotation/__init__.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/rotation\n",
            "  copying gptqmodel/quantization/rotation/hadamard_utils.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/rotation\n",
            "  copying gptqmodel/quantization/rotation/rotation.py -> build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/rotation\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  building 'gptqmodel_marlin_kernels' extension\n",
            "  creating build/temp.linux-x86_64-cpython-311/gptqmodel_ext/marlin\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/marlin/marlin_cuda.cpp -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/marlin/marlin_cuda.o -O3 -std=c++17 -fopenmp -lgomp -DENABLE_BF16 -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_marlin_kernels -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.\n",
            "  If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "    warnings.warn(\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/marlin/marlin_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/marlin/marlin_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_marlin_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      16 bytes stack frame, 32 bytes spill stores, 20 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 16 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      16 bytes stack frame, 60 bytes spill stores, 40 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 16 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      16 bytes stack frame, 60 bytes spill stores, 40 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 16 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 8 bytes spill stores, 4 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 210 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 210 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 212 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 206 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 138 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 138 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      160 bytes stack frame, 402 bytes spill stores, 712 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 160 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 244 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 162 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      48 bytes stack frame, 68 bytes spill stores, 56 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 48 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 252 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 194 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 132 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 132 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 130 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 118 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi4ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      200 bytes stack frame, 458 bytes spill stores, 620 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 200 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi3ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi2ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 232 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi128ELi1ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 164 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 194 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 148 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 142 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      192 bytes stack frame, 434 bytes spill stores, 596 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 192 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 238 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 182 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 20 bytes spill stores, 16 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      24 bytes stack frame, 40 bytes spill stores, 28 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 24 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 252 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 192 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 126 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 125 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 118 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi4ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      160 bytes stack frame, 414 bytes spill stores, 588 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 160 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi3ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi2ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 238 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li8ELi256ELi1ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 164 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      24 bytes stack frame, 44 bytes spill stores, 28 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 24 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 208 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 210 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 210 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 212 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 124 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 126 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 126 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      136 bytes stack frame, 434 bytes spill stores, 532 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 136 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 234 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 156 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 123 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi4ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      176 bytes stack frame, 474 bytes spill stores, 488 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 176 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi3ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi2ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 222 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi128ELi1ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 163 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 8 bytes spill stores, 4 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 202 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 148 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 140 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 140 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 142 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      160 bytes stack frame, 466 bytes spill stores, 496 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 160 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 224 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 164 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 8 bytes spill stores, 4 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 252 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 252 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 199 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 120 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 120 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 121 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi4ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      128 bytes stack frame, 446 bytes spill stores, 452 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 128 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi3ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi2ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 236 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI13__nv_bfloat16Li4ELi256ELi1ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 156 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      16 bytes stack frame, 32 bytes spill stores, 20 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 16 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      16 bytes stack frame, 60 bytes spill stores, 40 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 16 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      16 bytes stack frame, 60 bytes spill stores, 40 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 16 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 8 bytes spill stores, 4 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 210 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 210 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 212 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 206 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 138 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 138 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      160 bytes stack frame, 398 bytes spill stores, 700 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 160 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 244 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 162 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      48 bytes stack frame, 60 bytes spill stores, 56 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 48 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 252 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 192 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 132 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 132 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 126 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 118 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi4ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      208 bytes stack frame, 482 bytes spill stores, 648 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 208 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi3ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi2ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 232 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi128ELi1ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 161 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 194 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 148 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 142 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      184 bytes stack frame, 422 bytes spill stores, 584 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 184 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 238 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 182 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 20 bytes spill stores, 16 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      24 bytes stack frame, 40 bytes spill stores, 28 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 24 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 252 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 192 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 130 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 130 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 124 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 118 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi4ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      168 bytes stack frame, 438 bytes spill stores, 612 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 168 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi3ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi2ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 238 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi8ELi256ELi1ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 162 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 16 bytes spill stores, 12 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      24 bytes stack frame, 44 bytes spill stores, 28 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 24 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 208 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 210 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 210 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 212 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 124 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 126 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 126 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      136 bytes stack frame, 434 bytes spill stores, 472 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 136 bytes cumulative stack size, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 234 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi4ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 156 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 196 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 123 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi4ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      176 bytes stack frame, 474 bytes spill stores, 444 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 176 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi3ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi2ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 222 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi128ELi1ELi8ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 158 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 8 bytes spill stores, 4 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 198 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 202 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 148 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 140 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 140 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 142 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      160 bytes stack frame, 466 bytes spill stores, 436 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 160 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 224 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi8ELi8ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 164 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      8 bytes stack frame, 8 bytes spill stores, 4 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 8 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 252 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 253 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 252 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 200 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 199 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi8EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 120 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi4EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELi2EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 120 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb0ELb0ELin1EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 121 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi4ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      128 bytes stack frame, 446 bytes spill stores, 420 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 128 bytes cumulative stack size, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi3ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 433 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi2ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 236 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin6MarlinI6__halfLi4ELi256ELi1ELi16ELi4ELi4ELb1ELb0ELi0EEEvPK4int4S4_PS2_S5_S4_S4_PKiiiiiPib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 158 registers, 433 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin19permute_cols_kernelEPK4int4PKiPS0_iii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin19permute_cols_kernelEPK4int4PKiPS0_iii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 388 bytes cmem[0]\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/marlin/marlin_repack.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/marlin/marlin_repack.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_marlin_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin25gptq_marlin_repack_kernelILi256ELi8ELi32ELb1EEEvPKjS2_Pjii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin25gptq_marlin_repack_kernelILi256ELi8ELi32ELb1EEEvPKjS2_Pjii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 38 registers, 384 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin25gptq_marlin_repack_kernelILi256ELi8ELi32ELb0EEEvPKjS2_Pjii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin25gptq_marlin_repack_kernelILi256ELi8ELi32ELb0EEEvPKjS2_Pjii\n",
            "      32 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 32 bytes cumulative stack size, 384 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin25gptq_marlin_repack_kernelILi256ELi4ELi32ELb1EEEvPKjS2_Pjii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin25gptq_marlin_repack_kernelILi256ELi4ELi32ELb1EEEvPKjS2_Pjii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 38 registers, 384 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN6marlin25gptq_marlin_repack_kernelILi256ELi4ELi32ELb0EEEvPKjS2_Pjii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN6marlin25gptq_marlin_repack_kernelILi256ELi4ELi32ELb0EEEvPKjS2_Pjii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 29 registers, 384 bytes cmem[0]\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-311/gptqmodel_ext/marlin/marlin_cuda.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/marlin/marlin_cuda_kernel.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/marlin/marlin_repack.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/gptqmodel_marlin_kernels.cpython-311-x86_64-linux-gnu.so\n",
            "  building 'gptqmodel_qqq_kernels' extension\n",
            "  creating build/temp.linux-x86_64-cpython-311/gptqmodel_ext/qqq\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/qqq/qqq.cpp -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/qqq/qqq.o -O3 -std=c++17 -fopenmp -lgomp -DENABLE_BF16 -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_qqq_kernels -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/qqq/qqq_gemm.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/qqq/qqq_gemm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_qqq_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  gptqmodel_ext/qqq/qqq_gemm.cu(837): warning #177-D: variable \"tile_size\" was declared but never referenced\n",
            "    static constexpr int tile_size = 16;\n",
            "                         ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  gptqmodel_ext/qqq/qqq_gemm.cu(838): warning #177-D: variable \"max_par\" was declared but never referenced\n",
            "    static constexpr int max_par = 16;\n",
            "                         ^\n",
            "\n",
            "  gptqmodel_ext/qqq/qqq_gemm.cu(840): warning #177-D: variable \"pack_factor_4bit\" was declared but never referenced\n",
            "    static constexpr int pack_factor_4bit =\n",
            "                         ^\n",
            "\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi4ELi4ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi4ELi4ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      56 bytes stack frame, 88 bytes spill stores, 76 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 56 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi4ELi4ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi4ELi4ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      80 bytes stack frame, 140 bytes spill stores, 128 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 80 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi3ELi4ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi3ELi4ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      80 bytes stack frame, 92 bytes spill stores, 84 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 80 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi3ELi4ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi3ELi4ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      88 bytes stack frame, 116 bytes spill stores, 100 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 88 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi2ELi4ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi2ELi4ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi2ELi4ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi2ELi4ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 179 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi1ELi4ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi1ELi4ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 168 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi1ELi4ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi1ELi4ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 130 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi4ELi8ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi4ELi8ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      24 bytes stack frame, 96 bytes spill stores, 72 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 24 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi4ELi8ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi4ELi8ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      24 bytes stack frame, 96 bytes spill stores, 80 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 24 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi3ELi8ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi3ELi8ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi3ELi8ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi3ELi8ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi2ELi8ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi2ELi8ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 214 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi2ELi8ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi2ELi8ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 174 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi1ELi8ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi1ELi8ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi128ELi1ELi8ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi128ELi1ELi8ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi4ELi16ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi4ELi16ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      48 bytes stack frame, 64 bytes spill stores, 56 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 48 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi4ELi16ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi4ELi16ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      56 bytes stack frame, 84 bytes spill stores, 72 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 56 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi3ELi16ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi3ELi16ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 255 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi3ELi16ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi3ELi16ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi2ELi16ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi2ELi16ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 214 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi2ELi16ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi2ELi16ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 174 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi1ELi16ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi1ELi16ELi4ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 150 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi1ELi16ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi1ELi16ELi4ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi4ELi8ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi4ELi8ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      24 bytes stack frame, 96 bytes spill stores, 72 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 24 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi4ELi8ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi4ELi8ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      40 bytes stack frame, 160 bytes spill stores, 128 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 40 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi3ELi8ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi3ELi8ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      16 bytes stack frame, 64 bytes spill stores, 48 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 16 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi3ELi8ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi3ELi8ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      24 bytes stack frame, 96 bytes spill stores, 80 bytes spill loads\n",
            "  ptxas info    : Used 254 registers, 24 bytes cumulative stack size, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi2ELi8ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi2ELi8ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 250 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi2ELi8ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi2ELi8ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 182 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi1ELi8ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi1ELi8ELi8ELi4ELi8EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 166 registers, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z6MarlinILi256ELi1ELi8ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z6MarlinILi256ELi1ELi8ELi8ELi4ELin1EEvPK4int4S2_PS0_S3_PKfS2_S2_iiiPi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 432 bytes cmem[0]\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-311/gptqmodel_ext/qqq/qqq.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/qqq/qqq_gemm.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/gptqmodel_qqq_kernels.cpython-311-x86_64-linux-gnu.so\n",
            "  building 'gptqmodel_exllama_eora' extension\n",
            "  creating build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama_eora/eora\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllama_eora/eora/pybind.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama_eora/eora/pybind.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllama_eora -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllama_eora/eora/q_gemm.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama_eora/eora/q_gemm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllama_eora -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  gptqmodel_ext/exllama_eora/eora/q_gemm.cu(2008): warning #177-D: variable \"use_reconstruct\" was declared but never referenced\n",
            "        bool use_reconstruct = false;\n",
            "             ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq27make_sequential_8bit_kernelEPKjPjPKii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq27make_sequential_8bit_kernelEPKjPjPKii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 22 registers, 380 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq27make_sequential_3bit_kernelEPKjPjPKii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq27make_sequential_3bit_kernelEPKjPjPKii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 18 registers, 380 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq27make_sequential_2bit_kernelEPKjPjPKii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq27make_sequential_2bit_kernelEPKjPjPKii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 380 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq27make_sequential_4bit_kernelEPKjPjPKii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq27make_sequential_4bit_kernelEPKjPjPKii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 30 registers, 380 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq19shuffle_3bit_kernelEPjii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq19shuffle_3bit_kernelEPjii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 25 registers, 368 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq19shuffle_2bit_kernelEPjii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq19shuffle_2bit_kernelEPjii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 16 registers, 368 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq19shuffle_8bit_kernelEPjii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq19shuffle_8bit_kernelEPjii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 4 registers, 368 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq19shuffle_4bit_kernelEPjii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq19shuffle_4bit_kernelEPjii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 22 registers, 368 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq23reconstruct_gptq_kernelINS_17MatrixView_q8_rowELi8EEEvPKjPK6__halfS3_PKiiiiPS4_' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq23reconstruct_gptq_kernelINS_17MatrixView_q8_rowELi8EEEvPKjPK6__halfS3_PKiiiiPS4_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 28 registers, 408 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq23reconstruct_gptq_kernelINS_17MatrixView_q2_rowELi2EEEvPKjPK6__halfS3_PKiiiiPS4_' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq23reconstruct_gptq_kernelINS_17MatrixView_q2_rowELi2EEEvPKjPK6__halfS3_PKiiiiPS4_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 408 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq23reconstruct_gptq_kernelINS_17MatrixView_q4_rowELi4EEEvPKjPK6__halfS3_PKiiiiPS4_' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq23reconstruct_gptq_kernelINS_17MatrixView_q4_rowELi4EEEvPKjPK6__halfS3_PKiiiiPS4_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 408 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq28reconstruct_gptq_3bit_kernelEPKjPK6__halfS1_PKiiiiPS2_' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq28reconstruct_gptq_3bit_kernelEPKjPK6__halfS1_PKiiiiPS2_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 25 registers, 408 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq32gemm_half_q_half_alt_8bit_kernelEPK7__half2PKjP6__halfPKS5_S4_PKiiii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq32gemm_half_q_half_alt_8bit_kernelEPK7__half2PKjP6__halfPKS5_S4_PKiiii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 40 registers, 2048 bytes smem, 412 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq32gemm_half_q_half_alt_4bit_kernelEPK7__half2PKjP6__halfPKS5_S4_PKiiii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq32gemm_half_q_half_alt_4bit_kernelEPK7__half2PKjP6__halfPKS5_S4_PKiiii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 40 registers, 10240 bytes smem, 412 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq31reconstruct_exllama_2bit_kernelEPKjPKiS1_PK6__halfiiiPS4_' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq31reconstruct_exllama_2bit_kernelEPKjPKiS1_PK6__halfiiiPS4_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 512 bytes smem, 408 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq31reconstruct_exllama_3bit_kernelEPKjPKiS1_PK6__halfiiiPS4_' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq31reconstruct_exllama_3bit_kernelEPKjPKiS1_PK6__halfiiiPS4_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 96 registers, 512 bytes smem, 408 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq31reconstruct_exllama_4bit_kernelEPKjPKiS1_PK6__halfiiiPS4_' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq31reconstruct_exllama_4bit_kernelEPKjPKiS1_PK6__halfiiiPS4_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 56 registers, 512 bytes smem, 408 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq31reconstruct_exllama_8bit_kernelEPKjPKiS1_PK6__halfiiiPS4_' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq31reconstruct_exllama_8bit_kernelEPKjPKiS1_PK6__halfiiiPS4_\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 60 registers, 512 bytes smem, 408 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 95 registers, 2048 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 125 registers, 2048 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 98 registers, 2048 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 92 registers, 1792 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 123 registers, 1792 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 87 registers, 1792 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 80 registers, 1536 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 120 registers, 1536 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 84 registers, 1536 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 80 registers, 1280 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 118 registers, 1280 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 77 registers, 1280 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 71 registers, 1024 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 116 registers, 1024 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 76 registers, 1024 bytes smem, 436 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 70 registers, 768 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 94 registers, 768 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 71 registers, 768 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 58 registers, 512 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 86 registers, 512 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 63 registers, 512 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_4bit_kernel_eoraILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 48 registers, 256 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_3bit_kernel_eoraILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 256 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq38gemm_half_q_half_gptq_2bit_kernel_eoraILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKiS3_S3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 55 registers, 256 bytes smem, 436 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 2048 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 94 registers, 2048 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 120 registers, 2048 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi8EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 94 registers, 2048 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 112 registers, 1792 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 92 registers, 1792 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 118 registers, 1792 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi7EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 89 registers, 1792 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 108 registers, 1536 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 83 registers, 1536 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 117 registers, 1536 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi6EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 77 registers, 1536 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 102 registers, 1280 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 78 registers, 1280 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 114 registers, 1280 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi5EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 77 registers, 1280 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 96 registers, 1024 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 72 registers, 1024 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 114 registers, 1024 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi4EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 75 registers, 1024 bytes smem, 416 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 79 registers, 768 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 71 registers, 768 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 95 registers, 768 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi3EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 71 registers, 768 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 66 registers, 512 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 60 registers, 512 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 82 registers, 512 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi2EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 512 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_8bit_kernelILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 56 registers, 256 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_4bit_kernelILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 56 registers, 256 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_3bit_kernelILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 64 registers, 256 bytes smem, 416 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi' for 'sm_80'\n",
            "  ptxas info    : Function properties for _ZN4gptq33gemm_half_q_half_gptq_2bit_kernelILb1ELi1EEEvPK6__halfPKjS5_S3_PS1_iiiiPKi\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 48 registers, 256 bytes smem, 416 bytes cmem[0]\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama_eora/eora/pybind.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama_eora/eora/q_gemm.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/gptqmodel_exllama_eora.cpython-311-x86_64-linux-gnu.so\n",
            "  building 'gptqmodel_exllamav2_kernels' extension\n",
            "  creating build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllamav2/cuda\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllamav2/cuda/q_gemm.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllamav2/cuda/q_gemm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllamav2_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_Z12clear_kernelP6__halfii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z12clear_kernelP6__halfii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 6 registers, 368 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z28gemm_half_q_half_gptq_kernelILb1ELi8EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z28gemm_half_q_half_gptq_kernelILb1ELi8EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 94 registers, 2048 bytes smem, 429 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z28gemm_half_q_half_gptq_kernelILb1ELi7EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z28gemm_half_q_half_gptq_kernelILb1ELi7EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 94 registers, 1792 bytes smem, 429 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z28gemm_half_q_half_gptq_kernelILb1ELi6EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z28gemm_half_q_half_gptq_kernelILb1ELi6EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 80 registers, 1536 bytes smem, 429 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z28gemm_half_q_half_gptq_kernelILb1ELi5EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z28gemm_half_q_half_gptq_kernelILb1ELi5EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 73 registers, 1280 bytes smem, 429 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z28gemm_half_q_half_gptq_kernelILb1ELi4EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z28gemm_half_q_half_gptq_kernelILb1ELi4EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 70 registers, 1024 bytes smem, 429 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z28gemm_half_q_half_gptq_kernelILb1ELi3EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z28gemm_half_q_half_gptq_kernelILb1ELi3EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 66 registers, 768 bytes smem, 429 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z28gemm_half_q_half_gptq_kernelILb1ELi2EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z28gemm_half_q_half_gptq_kernelILb1ELi2EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 60 registers, 512 bytes smem, 429 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z28gemm_half_q_half_gptq_kernelILb1ELi1EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z28gemm_half_q_half_gptq_kernelILb1ELi1EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtib\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 56 registers, 256 bytes smem, 429 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z23gemm_half_q_half_kernelILb1ELi8EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23gemm_half_q_half_kernelILb1ELi8EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 64 bytes cumulative stack size, 2048 bytes smem, 449 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z23gemm_half_q_half_kernelILb1ELi7EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23gemm_half_q_half_kernelILb1ELi7EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 128 registers, 64 bytes cumulative stack size, 1792 bytes smem, 449 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z23gemm_half_q_half_kernelILb1ELi6EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23gemm_half_q_half_kernelILb1ELi6EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 122 registers, 64 bytes cumulative stack size, 1536 bytes smem, 449 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z23gemm_half_q_half_kernelILb1ELi5EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23gemm_half_q_half_kernelILb1ELi5EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 120 registers, 64 bytes cumulative stack size, 1280 bytes smem, 449 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z23gemm_half_q_half_kernelILb1ELi4EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23gemm_half_q_half_kernelILb1ELi4EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 118 registers, 64 bytes cumulative stack size, 1024 bytes smem, 449 bytes cmem[0], 8 bytes cmem[2]\n",
            "  ptxas info    : Compiling entry function '_Z23gemm_half_q_half_kernelILb1ELi3EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23gemm_half_q_half_kernelILb1ELi3EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 116 registers, 64 bytes cumulative stack size, 768 bytes smem, 449 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z23gemm_half_q_half_kernelILb1ELi2EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23gemm_half_q_half_kernelILb1ELi2EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 80 registers, 64 bytes cumulative stack size, 512 bytes smem, 449 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z23gemm_half_q_half_kernelILb1ELi1EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23gemm_half_q_half_kernelILb1ELi1EEvPK6__halfPKjS4_S2_PS0_iiiiiPKtiiiiiib\n",
            "      64 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 55 registers, 64 bytes cumulative stack size, 256 bytes smem, 449 bytes cmem[0]\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllamav2/cuda/q_matrix.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllamav2/cuda/q_matrix.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllamav2_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 0 bytes gmem\n",
            "  ptxas info    : Compiling entry function '_Z22make_sequential_kernelPKjPjPKtii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z22make_sequential_kernelPKjPjPKtii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 30 registers, 384 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z18reconstruct_kernelPKjPKtS0_PK6__halfiiiiPS3_iiiiii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z18reconstruct_kernelPKjPKtS0_PK6__halfiiiiPS3_iiiiii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 256 bytes smem, 432 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z23reconstruct_gptq_kernelPKjPKtS0_PK6__halfiiiiPS3_i' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z23reconstruct_gptq_kernelPKjPKtS0_PK6__halfiiiiPS3_i\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 48 registers, 256 bytes smem, 412 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z14shuffle_kernelPjiiiiiiii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z14shuffle_kernelPjiiiiiiii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 30 registers, 392 bytes cmem[0]\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllamav2/ext.cpp -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllamav2/ext.o -O3 -std=c++17 -fopenmp -lgomp -DENABLE_BF16 -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllamav2_kernels -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllamav2/cuda/q_gemm.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllamav2/cuda/q_matrix.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllamav2/ext.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/gptqmodel_exllamav2_kernels.cpython-311-x86_64-linux-gnu.so\n",
            "  building 'gptqmodel_exllama_kernels' extension\n",
            "  creating build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama\n",
            "  creating build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_func\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllama/cuda_buffers.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_buffers.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllama_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 0 bytes gmem\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllama/cuda_func/column_remap.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_func/column_remap.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllama_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 0 bytes gmem\n",
            "  ptxas info    : Compiling entry function '_Z19column_remap_kernelPK6__halfPS_iiPKj' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z19column_remap_kernelPK6__halfPS_iiPKj\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 24 registers, 384 bytes cmem[0]\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllama/cuda_func/q4_matmul.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_func/q4_matmul.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllama_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 6 bytes gmem, 48 bytes cmem[4]\n",
            "  ptxas info    : Compiling entry function '_Z16q4_matmul_kernelILb1ELb0ELb0EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z16q4_matmul_kernelILb1ELb0ELb0EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 425 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z16q4_matmul_kernelILb1ELb0ELb1EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z16q4_matmul_kernelILb1ELb0ELb1EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 425 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z16q4_matmul_kernelILb1ELb1ELb0EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z16q4_matmul_kernelILb1ELb1ELb0EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 425 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z16q4_matmul_kernelILb1ELb1ELb1EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z16q4_matmul_kernelILb1ELb1ELb1EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 425 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z16q4_matmul_kernelILb0ELb0ELb0EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z16q4_matmul_kernelILb0ELb0ELb0EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 425 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z16q4_matmul_kernelILb0ELb0ELb1EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z16q4_matmul_kernelILb0ELb0ELb1EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 425 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z16q4_matmul_kernelILb0ELb1ELb0EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z16q4_matmul_kernelILb0ELb1ELb0EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 425 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z16q4_matmul_kernelILb0ELb1ELb1EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z16q4_matmul_kernelILb0ELb1ELb1EEvPK6__halfPKjPS0_S2_S4_iiiiiS4_b\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 32 registers, 425 bytes cmem[0]\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllama/cuda_func/q4_matrix.cu -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_func/q4_matrix.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -D_GLIBCXX_USE_CXX11_ABI=0 --threads 8 --optimize=3 -lineinfo --resource-usage -Xfatbin -compress-all --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -diag-suppress=179,39 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllama_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n",
            "  ptxas info    : 0 bytes gmem\n",
            "  ptxas info    : Compiling entry function '_Z18reconstruct_kernelPKjP6__halfPKS1_S0_iii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z18reconstruct_kernelPKjP6__halfPKS1_S0_iii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 30 registers, 396 bytes cmem[0]\n",
            "  ptxas info    : Compiling entry function '_Z22make_sequential_kernelPKjPjS0_ii' for 'sm_80'\n",
            "  ptxas info    : Function properties for _Z22make_sequential_kernelPKjPjS0_ii\n",
            "      0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "  ptxas info    : Used 30 registers, 384 bytes cmem[0]\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -Igptqmodel_cuda -I/usr/include/python3.11 -c gptqmodel_ext/exllama/exllama_ext.cpp -o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/exllama_ext.o -O3 -std=c++17 -fopenmp -lgomp -DENABLE_BF16 -D_GLIBCXX_USE_CXX11_ABI=0 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=gptqmodel_exllama_kernels -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "  x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_buffers.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_func/column_remap.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_func/q4_matmul.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/cuda_func/q4_matrix.o build/temp.linux-x86_64-cpython-311/gptqmodel_ext/exllama/exllama_ext.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/gptqmodel_exllama_kernels.cpython-311-x86_64-linux-gnu.so\n",
            "  /usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "  !!\n",
            "\n",
            "          ********************************************************************************\n",
            "          Please avoid running ``setup.py`` directly.\n",
            "          Instead, use pypa/build, pypa/installer or other\n",
            "          standards-based tools.\n",
            "\n",
            "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "          ********************************************************************************\n",
            "\n",
            "  !!\n",
            "    self.initialize_options()\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/version.py -> build/bdist.linux-x86_64/wheel/./gptqmodel\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/adapter\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/adapter/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/adapter\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/adapter/adapter.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/adapter\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/adapter/peft.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/adapter\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/adapter/remote.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/adapter\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/eora\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/eora/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/eora\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/eora/eora.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/eora\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/dequantize_processor.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/eora_processor.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/gptq_processor.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/input_cache.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/loop_processor.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/module_looper.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/named_module.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/native_processor.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/looper/qqq_processor.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/looper\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/_const.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/auto.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/base.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/loader.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/writer.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/baichuan.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/base_qwen2_5_omni.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/base_qwen2_vl.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/bloom.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/chatglm.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/codegen.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/cohere.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/cohere2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/dbrx.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/dbrx_converted.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/decilm.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/deepseek_v2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/deepseek_v3.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/dream.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/exaone.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/falcon_h1.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/gemma.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/gemma2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/gemma3.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/glm.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/gpt2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/gpt_bigcode.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/gpt_neox.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/gptj.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/granite.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/grinmoe.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/hymba.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/instella.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/internlm.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/internlm2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/llama.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/longllama.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/mimo.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/minicpm.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/minicpm3.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/mistral.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/mixtral.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/mllama.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/mobilellm.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/moss.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/mpt.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/olmo2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/opt.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/ovis.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/phi.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/phi3.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/phi4.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/qwen.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/qwen2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/qwen2_5_omni.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/qwen2_5_vl.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/qwen2_moe.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/qwen2_vl.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/qwen3.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/qwen3_moe.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/rw.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/stablelmepoch.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/starcoder2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/telechat2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/xverse.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/models/definitions/yi.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/models/definitions\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/nn_modules\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/hooked_linear.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/bitblas.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/bitblas_target_detector.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/exllama.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/exllama_eora.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/exllamav2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/ipex.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/marlin.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/qqq.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/torch.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/tritonv2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/qlinear/utils.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/qlinear\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/nn_modules/triton_utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/triton_utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils/custom_autotune.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/triton_utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils/dequant.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/triton_utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils/kernels.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/triton_utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/nn_modules/triton_utils/mixin.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/nn_modules/triton_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/quantization\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/config.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/gptq.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/gptqv2.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/qqq.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/quantizer.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/quantization/rotation\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/rotation/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization/rotation\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/rotation/hadamard_utils.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization/rotation\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/quantization/rotation/rotation.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/quantization/rotation\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/__init__.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/backend.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/bitblas.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/calibration.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/data.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/device.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/eval.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/evalplus.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/exllama.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/hf.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/image.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/importer.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/logger.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/marlin.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/mlx.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/mmlupro.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/model.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/openai_server.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/perplexity.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/plotly.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/python.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/rocm.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/safetensor.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/sglang.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/tensor.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/terminal.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/torch.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/vllm.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel/utils/vram.py -> build/bdist.linux-x86_64/wheel/./gptqmodel/utils\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel_marlin_kernels.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel_qqq_kernels.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel_exllama_eora.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel_exllamav2_kernels.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  copying build/lib.linux-x86_64-cpython-311/gptqmodel_exllama_kernels.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating gptqmodel.egg-info\n",
            "  writing gptqmodel.egg-info/PKG-INFO\n",
            "  writing dependency_links to gptqmodel.egg-info/dependency_links.txt\n",
            "  writing requirements to gptqmodel.egg-info/requires.txt\n",
            "  writing top-level names to gptqmodel.egg-info/top_level.txt\n",
            "  writing manifest file 'gptqmodel.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'gptqmodel.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  no previously-included directories found matching 'format'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'gptqmodel.egg-info/SOURCES.txt'\n",
            "  Copying gptqmodel.egg-info to build/bdist.linux-x86_64/wheel/./gptqmodel-4.0.0.dev0-py3.11.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/gptqmodel-4.0.0.dev0.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-1_ghlcpz/gptqmodel-4.0.0.dev0-cp311-cp311-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'gptqmodel_exllama_eora.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'gptqmodel_exllama_kernels.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'gptqmodel_exllamav2_kernels.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'gptqmodel_marlin_kernels.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'gptqmodel_qqq_kernels.cpython-311-x86_64-linux-gnu.so'\n",
            "  adding 'gptqmodel/__init__.py'\n",
            "  adding 'gptqmodel/version.py'\n",
            "  adding 'gptqmodel/adapter/__init__.py'\n",
            "  adding 'gptqmodel/adapter/adapter.py'\n",
            "  adding 'gptqmodel/adapter/peft.py'\n",
            "  adding 'gptqmodel/adapter/remote.py'\n",
            "  adding 'gptqmodel/eora/__init__.py'\n",
            "  adding 'gptqmodel/eora/eora.py'\n",
            "  adding 'gptqmodel/looper/__init__.py'\n",
            "  adding 'gptqmodel/looper/dequantize_processor.py'\n",
            "  adding 'gptqmodel/looper/eora_processor.py'\n",
            "  adding 'gptqmodel/looper/gptq_processor.py'\n",
            "  adding 'gptqmodel/looper/input_cache.py'\n",
            "  adding 'gptqmodel/looper/loop_processor.py'\n",
            "  adding 'gptqmodel/looper/module_looper.py'\n",
            "  adding 'gptqmodel/looper/named_module.py'\n",
            "  adding 'gptqmodel/looper/native_processor.py'\n",
            "  adding 'gptqmodel/looper/qqq_processor.py'\n",
            "  adding 'gptqmodel/models/__init__.py'\n",
            "  adding 'gptqmodel/models/_const.py'\n",
            "  adding 'gptqmodel/models/auto.py'\n",
            "  adding 'gptqmodel/models/base.py'\n",
            "  adding 'gptqmodel/models/loader.py'\n",
            "  adding 'gptqmodel/models/writer.py'\n",
            "  adding 'gptqmodel/models/definitions/__init__.py'\n",
            "  adding 'gptqmodel/models/definitions/baichuan.py'\n",
            "  adding 'gptqmodel/models/definitions/base_qwen2_5_omni.py'\n",
            "  adding 'gptqmodel/models/definitions/base_qwen2_vl.py'\n",
            "  adding 'gptqmodel/models/definitions/bloom.py'\n",
            "  adding 'gptqmodel/models/definitions/chatglm.py'\n",
            "  adding 'gptqmodel/models/definitions/codegen.py'\n",
            "  adding 'gptqmodel/models/definitions/cohere.py'\n",
            "  adding 'gptqmodel/models/definitions/cohere2.py'\n",
            "  adding 'gptqmodel/models/definitions/dbrx.py'\n",
            "  adding 'gptqmodel/models/definitions/dbrx_converted.py'\n",
            "  adding 'gptqmodel/models/definitions/decilm.py'\n",
            "  adding 'gptqmodel/models/definitions/deepseek_v2.py'\n",
            "  adding 'gptqmodel/models/definitions/deepseek_v3.py'\n",
            "  adding 'gptqmodel/models/definitions/dream.py'\n",
            "  adding 'gptqmodel/models/definitions/exaone.py'\n",
            "  adding 'gptqmodel/models/definitions/falcon_h1.py'\n",
            "  adding 'gptqmodel/models/definitions/gemma.py'\n",
            "  adding 'gptqmodel/models/definitions/gemma2.py'\n",
            "  adding 'gptqmodel/models/definitions/gemma3.py'\n",
            "  adding 'gptqmodel/models/definitions/glm.py'\n",
            "  adding 'gptqmodel/models/definitions/gpt2.py'\n",
            "  adding 'gptqmodel/models/definitions/gpt_bigcode.py'\n",
            "  adding 'gptqmodel/models/definitions/gpt_neox.py'\n",
            "  adding 'gptqmodel/models/definitions/gptj.py'\n",
            "  adding 'gptqmodel/models/definitions/granite.py'\n",
            "  adding 'gptqmodel/models/definitions/grinmoe.py'\n",
            "  adding 'gptqmodel/models/definitions/hymba.py'\n",
            "  adding 'gptqmodel/models/definitions/instella.py'\n",
            "  adding 'gptqmodel/models/definitions/internlm.py'\n",
            "  adding 'gptqmodel/models/definitions/internlm2.py'\n",
            "  adding 'gptqmodel/models/definitions/llama.py'\n",
            "  adding 'gptqmodel/models/definitions/longllama.py'\n",
            "  adding 'gptqmodel/models/definitions/mimo.py'\n",
            "  adding 'gptqmodel/models/definitions/minicpm.py'\n",
            "  adding 'gptqmodel/models/definitions/minicpm3.py'\n",
            "  adding 'gptqmodel/models/definitions/mistral.py'\n",
            "  adding 'gptqmodel/models/definitions/mixtral.py'\n",
            "  adding 'gptqmodel/models/definitions/mllama.py'\n",
            "  adding 'gptqmodel/models/definitions/mobilellm.py'\n",
            "  adding 'gptqmodel/models/definitions/moss.py'\n",
            "  adding 'gptqmodel/models/definitions/mpt.py'\n",
            "  adding 'gptqmodel/models/definitions/olmo2.py'\n",
            "  adding 'gptqmodel/models/definitions/opt.py'\n",
            "  adding 'gptqmodel/models/definitions/ovis.py'\n",
            "  adding 'gptqmodel/models/definitions/phi.py'\n",
            "  adding 'gptqmodel/models/definitions/phi3.py'\n",
            "  adding 'gptqmodel/models/definitions/phi4.py'\n",
            "  adding 'gptqmodel/models/definitions/qwen.py'\n",
            "  adding 'gptqmodel/models/definitions/qwen2.py'\n",
            "  adding 'gptqmodel/models/definitions/qwen2_5_omni.py'\n",
            "  adding 'gptqmodel/models/definitions/qwen2_5_vl.py'\n",
            "  adding 'gptqmodel/models/definitions/qwen2_moe.py'\n",
            "  adding 'gptqmodel/models/definitions/qwen2_vl.py'\n",
            "  adding 'gptqmodel/models/definitions/qwen3.py'\n",
            "  adding 'gptqmodel/models/definitions/qwen3_moe.py'\n",
            "  adding 'gptqmodel/models/definitions/rw.py'\n",
            "  adding 'gptqmodel/models/definitions/stablelmepoch.py'\n",
            "  adding 'gptqmodel/models/definitions/starcoder2.py'\n",
            "  adding 'gptqmodel/models/definitions/telechat2.py'\n",
            "  adding 'gptqmodel/models/definitions/xverse.py'\n",
            "  adding 'gptqmodel/models/definitions/yi.py'\n",
            "  adding 'gptqmodel/nn_modules/__init__.py'\n",
            "  adding 'gptqmodel/nn_modules/hooked_linear.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/__init__.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/bitblas.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/bitblas_target_detector.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/exllama.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/exllama_eora.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/exllamav2.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/ipex.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/marlin.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/qqq.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/torch.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/tritonv2.py'\n",
            "  adding 'gptqmodel/nn_modules/qlinear/utils.py'\n",
            "  adding 'gptqmodel/nn_modules/triton_utils/__init__.py'\n",
            "  adding 'gptqmodel/nn_modules/triton_utils/custom_autotune.py'\n",
            "  adding 'gptqmodel/nn_modules/triton_utils/dequant.py'\n",
            "  adding 'gptqmodel/nn_modules/triton_utils/kernels.py'\n",
            "  adding 'gptqmodel/nn_modules/triton_utils/mixin.py'\n",
            "  adding 'gptqmodel/quantization/__init__.py'\n",
            "  adding 'gptqmodel/quantization/config.py'\n",
            "  adding 'gptqmodel/quantization/gptq.py'\n",
            "  adding 'gptqmodel/quantization/gptqv2.py'\n",
            "  adding 'gptqmodel/quantization/qqq.py'\n",
            "  adding 'gptqmodel/quantization/quantizer.py'\n",
            "  adding 'gptqmodel/quantization/rotation/__init__.py'\n",
            "  adding 'gptqmodel/quantization/rotation/hadamard_utils.py'\n",
            "  adding 'gptqmodel/quantization/rotation/rotation.py'\n",
            "  adding 'gptqmodel/utils/__init__.py'\n",
            "  adding 'gptqmodel/utils/backend.py'\n",
            "  adding 'gptqmodel/utils/bitblas.py'\n",
            "  adding 'gptqmodel/utils/calibration.py'\n",
            "  adding 'gptqmodel/utils/data.py'\n",
            "  adding 'gptqmodel/utils/device.py'\n",
            "  adding 'gptqmodel/utils/eval.py'\n",
            "  adding 'gptqmodel/utils/evalplus.py'\n",
            "  adding 'gptqmodel/utils/exllama.py'\n",
            "  adding 'gptqmodel/utils/hf.py'\n",
            "  adding 'gptqmodel/utils/image.py'\n",
            "  adding 'gptqmodel/utils/importer.py'\n",
            "  adding 'gptqmodel/utils/logger.py'\n",
            "  adding 'gptqmodel/utils/marlin.py'\n",
            "  adding 'gptqmodel/utils/mlx.py'\n",
            "  adding 'gptqmodel/utils/mmlupro.py'\n",
            "  adding 'gptqmodel/utils/model.py'\n",
            "  adding 'gptqmodel/utils/openai_server.py'\n",
            "  adding 'gptqmodel/utils/perplexity.py'\n",
            "  adding 'gptqmodel/utils/plotly.py'\n",
            "  adding 'gptqmodel/utils/python.py'\n",
            "  adding 'gptqmodel/utils/rocm.py'\n",
            "  adding 'gptqmodel/utils/safetensor.py'\n",
            "  adding 'gptqmodel/utils/sglang.py'\n",
            "  adding 'gptqmodel/utils/tensor.py'\n",
            "  adding 'gptqmodel/utils/terminal.py'\n",
            "  adding 'gptqmodel/utils/torch.py'\n",
            "  adding 'gptqmodel/utils/vllm.py'\n",
            "  adding 'gptqmodel/utils/vram.py'\n",
            "  adding 'gptqmodel-4.0.0.dev0.dist-info/LICENSE'\n",
            "  adding 'gptqmodel-4.0.0.dev0.dist-info/METADATA'\n",
            "  adding 'gptqmodel-4.0.0.dev0.dist-info/WHEEL'\n",
            "  adding 'gptqmodel-4.0.0.dev0.dist-info/top_level.txt'\n",
            "  adding 'gptqmodel-4.0.0.dev0.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for gptqmodel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gptqmodel: filename=gptqmodel-4.0.0.dev0-cp311-cp311-linux_x86_64.whl size=42639857 sha256=ce4a37ea526797b7e61a32402d95b7c8a4474f5b0dab3d78256f961be33b1a9b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_kg5259t/wheels/a4/4e/e3/b9bab633c8feb2f89b0f05ae758e01f906ec8a3668c3a6b0c4\n",
            "Successfully built gptqmodel\n",
            "Installing collected packages: gptqmodel\n",
            "Successfully installed gptqmodel-4.0.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall flash_attn -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TInUp5D2sWqc",
        "outputId": "c54b3b07-bdc8-4f1c-8150-f7ddd6f127d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: flash_attn 2.8.0.post2\n",
            "Uninstalling flash_attn-2.8.0.post2:\n",
            "  Successfully uninstalled flash_attn-2.8.0.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip cache purge\n",
        "# !pip install transformers==4.52.3 accelerate\n",
        "!MAX_JOBS=4 pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO9nQqZ8gjQe",
        "outputId": "954d48a5-d892-41d4-a0ca-8dee43ad7c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.0.post2.tar.gz (7.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m6.7/7.9 MB\u001b[0m \u001b[31m200.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.0.post2-cp311-cp311-linux_x86_64.whl size=255936976 sha256=e7e8fd0d50112e3302a96fcd5b5ae4606bd38c20a1149fc666fed5c87ba5a9c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/75/55/57ba1e272fd7fa1a01d9ba6b5334b7adaabf79900ede22c040\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.0.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "libraries = [\n",
        "    \"torch\",\n",
        "    \"torchvision\",\n",
        "    \"torchaudio\",\n",
        "    \"datasets\",\n",
        "    \"transformers\",\n",
        "    \"accelerate\",\n",
        "    \"peft\",\n",
        "    \"trl\",\n",
        "    \"deepspeed\",\n",
        "    \"bitsandbytes\",\n",
        "    \"flash-attn\",\n",
        "    \"flash-attn\"\n",
        "]\n",
        "\n",
        "print(\"📦 Installed Library Versions:\\n\")\n",
        "\n",
        "for lib in libraries:\n",
        "    try:\n",
        "        module = importlib.import_module(lib)\n",
        "        version = getattr(module, \"__version__\", \"Version info not available\")\n",
        "        print(f\"{lib:<15} : {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"{lib:<15} : ❌ Not Installed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVSNtWgsv2u7",
        "outputId": "60b18faa-8e96-4c40-a276-37d503112d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installed Library Versions:\n",
            "\n",
            "torch           : 2.6.0+cu124\n",
            "torchvision     : 0.21.0+cu124\n",
            "torchaudio      : 2.6.0+cu124\n",
            "datasets        : 3.6.0\n",
            "transformers    : 4.52.3\n",
            "accelerate      : 1.7.0\n",
            "peft            : ❌ Not Installed\n",
            "trl             : 0.9.6\n",
            "deepspeed       : ❌ Not Installed\n",
            "bitsandbytes    : ❌ Not Installed\n",
            "flash-attn      : ❌ Not Installed\n",
            "flash-attn      : ❌ Not Installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from gptqmodel import GPTQModel, QuantizeConfig\n",
        "import json\n",
        "\n",
        "model_id = '/content/drive/MyDrive/sarvam-ai/LLaMA-Factory/output/qwen2_5vl_7b_lora_merged_sarvam'\n",
        "quant_path = '/content/drive/MyDrive/sarvam-ai/qwen2_5vl_7b_lora_awq'\n",
        "txt_file_path = '/content/drive/MyDrive/sarvam-ai/dataset/calider_dataset.json'\n",
        "\n",
        "with open(txt_file_path, 'r') as f:\n",
        "    message = json.load(f)\n",
        "\n",
        "\n",
        "quant_config = QuantizeConfig(bits=4, group_size=128)\n",
        "\n",
        "model = GPTQModel.load(model_id, quant_config)\n",
        "\n",
        "# increase `batch_size` to match gpu/vram specs to speed up quantization\n",
        "model.quantize(message, batch_size=1)\n",
        "\n",
        "model.save(quant_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2ef13a50f3a54b8c8ce3430e66fbb5f1",
            "7778c0a3a37b423393eaf624ab53a1df",
            "8a794ac5f79b4ce281bc14ded2abf9fa",
            "4363529174364421809643075f82c046",
            "aa3d58de94f84587889756b37816edda",
            "f1d6a71abda94aec8143fbad19080dd7",
            "c90bc9bff3d24d579e4e5f8d9b57e6d1",
            "d6415d38efe044ed8cd79a6293cd1e39",
            "29829c8949d142bc9af916c33241129b",
            "8544f7400e954b43b9f0d3bb496f15ad",
            "b5112783fa9c428b88b9c90bcc6795c9"
          ]
        },
        "id": "3Wj_TbN2VkHx",
        "outputId": "a1f370ef-00b3-4d33-b04d-daaa02040adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
            "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native bfloat16): `torch.bfloat16`                   \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ef13a50f3a54b8c8ce3430e66fbb5f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 1e-06\n",
            "}\n",
            "\n",
            "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n",
            "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
            "\u001b[33mWARN\u001b[0m  Calibration dataset size should be more than 256. Current: 20.           \n",
            "\u001b[32mINFO\u001b[0m  Hooked Modules: Using tree based config for accurate targeting of modules\n",
            "\u001b[33mWARN\u001b[0m  Feature `Torch Compile` requires python GIL. Feature is currently skipped/disabled.\n",
            "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_tussor_time_06_15_2025_18h_27m_30s.log`\n",
            "\u001b[32mINFO\u001b[0m  -------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram     |\n",
            "\u001b[32mINFO\u001b[0m  -------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[92m0.0000017324\u001b[0m | 108881      | 0.05000     | 1.536     | 0.956        | 806.57MB     | \n",
            "\u001b[32mINFO\u001b[0m  -------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.0000002908\u001b[0m | 108881      | 0.05000     | 1.320     | 0.956        | 757.70MB     | \n",
            "\u001b[32mINFO\u001b[0m  -------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[92m0.0000082384\u001b[0m | 108881      | 0.05000     | 1.330     | 0.956        | 730.53MB     | \n",
            "\u001b[32mINFO\u001b[0m  -------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.0000002639\u001b[0m | 108881      | 0.05000     | 1.334     | 0.649        | 944.71MB     | \n",
            "\u001b[32mINFO\u001b[0m  -------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[92m0.0000163912\u001b[0m | 108881      | 0.05000     | 1.378     | 0.779        | 1316.27MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[92m0.0000369028\u001b[0m | 108881      | 0.05000     | 1.358     | 0.779        | 1271.33MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[92m0.0000024251\u001b[0m | 108881      | 0.05000     | 8.469     | 4.403        | 1485.37MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram      |\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[92m0.0000030558\u001b[0m | 108881      | 0.05000     | 1.331     | 0.744        | 2633.82MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.0000009130\u001b[0m | 108881      | 0.05000     | 1.324     | 0.744        | 2584.95MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[92m0.0000114261\u001b[0m | 108881      | 0.05000     | 1.324     | 0.744        | 2557.75MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.0000002752\u001b[0m | 108881      | 0.05000     | 1.340     | 0.499        | 2768.52MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[92m0.0002835290\u001b[0m | 108881      | 0.05000     | 1.385     | 0.618        | 3137.60MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[92m0.0004102985\u001b[0m | 108881      | 0.05000     | 1.353     | 0.618        | 3093.85MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[92m0.0000038933\u001b[0m | 108881      | 0.05000     | 8.339     | 4.213        | 3313.32MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram      |\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[92m0.0000063345\u001b[0m | 108881      | 0.05000     | 1.335     | 0.743        | 3715.02MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.0000012121\u001b[0m | 108881      | 0.05000     | 1.296     | 0.743        | 3666.00MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[92m0.0000239803\u001b[0m | 108881      | 0.05000     | 1.339     | 0.743        | 3637.89MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.0000010365\u001b[0m | 108881      | 0.05000     | 1.327     | 0.495        | 3849.81MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[92m0.0002853052\u001b[0m | 108881      | 0.05000     | 1.374     | 0.619        | 4218.46MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[92m0.0004318822\u001b[0m | 108881      | 0.05000     | 1.338     | 0.619        | 4172.63MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.0000108665\u001b[0m | 108881      | 0.05000     | 8.398     | 4.225        | 4391.63MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram      |\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[92m0.0000074644\u001b[0m | 108881      | 0.05000     | 1.339     | 0.742        | 4792.60MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.0000019818\u001b[0m | 108881      | 0.05000     | 1.295     | 0.742        | 4744.04MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[92m0.0000293808\u001b[0m | 108881      | 0.05000     | 1.338     | 0.742        | 4717.32MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.0000019861\u001b[0m | 108881      | 0.05000     | 1.340     | 0.511        | 4930.68MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[92m0.0008203831\u001b[0m | 108881      | 0.05000     | 1.373     | 0.619        | 5297.12MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[92m0.0010389214\u001b[0m | 108881      | 0.05000     | 1.355     | 0.619        | 5252.18MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.0000141645\u001b[0m | 108881      | 0.05000     | 8.424     | 4.214        | 5471.97MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram      |\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[92m0.0000129742\u001b[0m | 108881      | 0.05000     | 1.348     | 0.743        | 5869.91MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.0000046115\u001b[0m | 108881      | 0.05000     | 1.316     | 0.743        | 5821.35MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[92m0.0000579187\u001b[0m | 108881      | 0.05000     | 1.337     | 0.743        | 5793.91MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.0000016409\u001b[0m | 108881      | 0.05000     | 1.349     | 0.500        | 6009.09MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[92m0.0007219147\u001b[0m | 108881      | 0.05000     | 1.378     | 0.618        | 6374.76MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[92m0.0010085503\u001b[0m | 108881      | 0.05000     | 1.356     | 0.618        | 6329.98MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.down_proj        | \u001b[92m0.0000143196\u001b[0m | 108881      | 0.05000     | 8.442     | 4.233        | 6548.17MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram      |\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[92m0.0000125100\u001b[0m | 108881      | 0.05000     | 1.341     | 0.744        | 6949.71MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[92m0.0000052851\u001b[0m | 108881      | 0.05000     | 1.314     | 0.744        | 6900.44MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[92m0.0000613747\u001b[0m | 108881      | 0.05000     | 1.333     | 0.744        | 6872.91MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.o_proj     | \u001b[92m0.0000016976\u001b[0m | 108881      | 0.05000     | 1.347     | 0.502        | 7085.03MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.up_proj          | \u001b[92m0.0011398092\u001b[0m | 108881      | 0.05000     | 1.379     | 0.619        | 7453.77MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.gate_proj        | \u001b[92m0.0013424374\u001b[0m | 108881      | 0.05000     | 1.360     | 0.619        | 7408.81MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.down_proj        | \u001b[92m0.0000144286\u001b[0m | 108881      | 0.05000     | 8.405     | 4.234        | 7629.87MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram      |\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[92m0.0000094914\u001b[0m | 108881      | 0.05000     | 1.336     | 0.743        | 8028.02MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[92m0.0000048484\u001b[0m | 108881      | 0.05000     | 1.310     | 0.743        | 7978.48MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[92m0.0000474975\u001b[0m | 108881      | 0.05000     | 1.338     | 0.743        | 7951.76MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.o_proj     | \u001b[92m0.0000020938\u001b[0m | 108881      | 0.05000     | 1.350     | 0.501        | 8169.82MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.up_proj          | \u001b[92m0.0002430593\u001b[0m | 108881      | 0.05000     | 1.381     | 0.618        | 8532.83MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.gate_proj        | \u001b[92m0.0003279317\u001b[0m | 108881      | 0.05000     | 1.367     | 0.618        | 8487.89MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.down_proj        | \u001b[92m0.0000233010\u001b[0m | 108881      | 0.05000     | 8.458     | 4.222        | 8710.44MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram      |\n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[92m0.0000095833\u001b[0m | 108881      | 0.05000     | 1.334     | 0.743        | 9109.28MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[92m0.0000087360\u001b[0m | 108881      | 0.05000     | 1.311     | 0.743        | 9059.73MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[92m0.0000543762\u001b[0m | 108881      | 0.05000     | 1.337     | 0.743        | 9032.44MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.o_proj     | \u001b[92m0.0000058821\u001b[0m | 108881      | 0.05000     | 1.351     | 0.490        | 9248.67MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.up_proj          | \u001b[92m0.0002131708\u001b[0m | 108881      | 0.05000     | 1.384     | 0.619        | 9612.88MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.gate_proj        | \u001b[92m0.0002294698\u001b[0m | 108881      | 0.05000     | 1.366     | 0.619        | 9567.86MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.down_proj        | \u001b[92m0.0000255592\u001b[0m | 108881      | 0.05000     | 8.430     | 4.220        | 9788.18MB     | \n",
            "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[92m0.0000174204\u001b[0m | 108881      | 0.05000     | 1.337     | 0.742        | 10186.42MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[92m0.0000071506\u001b[0m | 108881      | 0.05000     | 1.300     | 0.742        | 10137.73MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[92m0.0000788454\u001b[0m | 108881      | 0.05000     | 1.363     | 0.742        | 10110.35MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.o_proj     | \u001b[92m0.0000063189\u001b[0m | 108881      | 0.05000     | 1.345     | 0.515        | 10325.88MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.up_proj          | \u001b[92m0.0002156751\u001b[0m | 108881      | 0.05000     | 1.393     | 0.618        | 10694.25MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.gate_proj        | \u001b[92m0.0002202717\u001b[0m | 108881      | 0.05000     | 1.376     | 0.618        | 10649.31MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.down_proj        | \u001b[92m0.0000244108\u001b[0m | 108881      | 0.05000     | 8.406     | 4.231        | 10869.09MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[92m0.0000127604\u001b[0m | 108881      | 0.05000     | 1.328     | 0.743        | 11265.51MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[92m0.0000099683\u001b[0m | 108881      | 0.05000     | 1.301     | 0.743        | 11216.23MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[92m0.0000704870\u001b[0m | 108881      | 0.05000     | 1.341     | 0.743        | 11189.03MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[92m0.0000055304\u001b[0m | 108881      | 0.05000     | 1.347     | 0.496        | 11401.09MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[92m0.0004303468\u001b[0m | 108881      | 0.05000     | 1.386     | 0.619        | 11770.68MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[92m0.0006615328\u001b[0m | 108881      | 0.05000     | 1.374     | 0.619        | 11725.74MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[92m0.0000212011\u001b[0m | 108881      | 0.05000     | 8.434     | 4.208        | 11943.58MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[92m0.0000122280\u001b[0m | 108881      | 0.05000     | 1.342     | 0.743        | 12345.13MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[92m0.0000066516\u001b[0m | 108881      | 0.05000     | 1.299     | 0.743        | 12295.96MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[92m0.0000628042\u001b[0m | 108881      | 0.05000     | 1.341     | 0.743        | 12268.34MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[92m0.0000047659\u001b[0m | 108881      | 0.05000     | 1.363     | 0.506        | 12480.34MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[92m0.0002083560\u001b[0m | 108881      | 0.05000     | 1.387     | 0.619        | 12852.89MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[92m0.0002286565\u001b[0m | 108881      | 0.05000     | 1.351     | 0.619        | 12808.77MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[92m0.0000213229\u001b[0m | 108881      | 0.05000     | 8.398     | 4.233        | 13025.44MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[92m0.0000152690\u001b[0m | 108881      | 0.05000     | 1.331     | 0.742        | 13423.20MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[92m0.0000059668\u001b[0m | 108881      | 0.05000     | 1.314     | 0.742        | 13373.65MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[92m0.0000676276\u001b[0m | 108881      | 0.05000     | 1.339     | 0.742        | 13346.46MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[92m0.0000062683\u001b[0m | 108881      | 0.05000     | 1.341     | 0.517        | 13557.21MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.up_proj          | \u001b[92m0.0001895227\u001b[0m | 108881      | 0.05000     | 1.383     | 0.620        | 13930.05MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.gate_proj        | \u001b[92m0.0001940822\u001b[0m | 108881      | 0.05000     | 1.359     | 0.620        | 13885.38MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.down_proj        | \u001b[92m0.0000194851\u001b[0m | 108881      | 0.05000     | 8.503     | 4.232        | 14103.76MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[92m0.0000157225\u001b[0m | 108881      | 0.05000     | 1.328     | 0.742        | 14504.01MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[92m0.0000075115\u001b[0m | 108881      | 0.05000     | 1.315     | 0.742        | 14454.51MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[92m0.0000724183\u001b[0m | 108881      | 0.05000     | 1.340     | 0.742        | 14426.41MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.o_proj     | \u001b[92m0.0000062006\u001b[0m | 108881      | 0.05000     | 1.401     | 0.517        | 14636.37MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.up_proj          | \u001b[92m0.0001923635\u001b[0m | 108881      | 0.05000     | 1.382     | 0.619        | 15009.55MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.gate_proj        | \u001b[92m0.0001863338\u001b[0m | 108881      | 0.05000     | 1.358     | 0.619        | 14964.62MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.down_proj        | \u001b[92m0.0000267591\u001b[0m | 108881      | 0.05000     | 8.466     | 4.237        | 15183.29MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[92m0.0000145638\u001b[0m | 108881      | 0.05000     | 1.341     | 0.742        | 15582.12MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[92m0.0000097223\u001b[0m | 108881      | 0.05000     | 1.349     | 0.742        | 15532.53MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[92m0.0000790993\u001b[0m | 108881      | 0.05000     | 1.336     | 0.742        | 15505.21MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.o_proj     | \u001b[92m0.0000110796\u001b[0m | 108881      | 0.05000     | 1.341     | 0.505        | 15716.47MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.up_proj          | \u001b[92m0.0001912182\u001b[0m | 108881      | 0.05000     | 1.377     | 0.618        | 16088.21MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.gate_proj        | \u001b[92m0.0001988703\u001b[0m | 108881      | 0.05000     | 1.359     | 0.618        | 16044.81MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.down_proj        | \u001b[92m0.0000248165\u001b[0m | 108881      | 0.05000     | 8.427     | 4.239        | 16262.46MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[92m0.0000196858\u001b[0m | 108881      | 0.05000     | 1.334     | 0.743        | 16661.37MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[92m0.0000088846\u001b[0m | 108881      | 0.05000     | 1.324     | 0.743        | 16611.76MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[92m0.0001043282\u001b[0m | 108881      | 0.05000     | 1.371     | 0.743        | 16584.47MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.o_proj     | \u001b[92m0.0000135083\u001b[0m | 108881      | 0.05000     | 1.346     | 0.509        | 16793.51MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.up_proj          | \u001b[92m0.0002141096\u001b[0m | 108881      | 0.05000     | 1.382     | 0.618        | 17166.57MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.gate_proj        | \u001b[92m0.0002108538\u001b[0m | 108881      | 0.05000     | 1.353     | 0.618        | 17122.40MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.down_proj        | \u001b[92m0.0000306765\u001b[0m | 108881      | 0.05000     | 8.432     | 4.242        | 17342.75MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[92m0.0000190205\u001b[0m | 108881      | 0.05000     | 1.345     | 0.742        | 17742.20MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[92m0.0000083397\u001b[0m | 108881      | 0.05000     | 1.331     | 0.742        | 17692.68MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[92m0.0000891013\u001b[0m | 108881      | 0.05000     | 1.343     | 0.742        | 17665.48MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.o_proj     | \u001b[92m0.0000129725\u001b[0m | 108881      | 0.05000     | 1.333     | 0.497        | 17872.48MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.up_proj          | \u001b[92m0.0002104531\u001b[0m | 108881      | 0.05000     | 1.385     | 0.619        | 18247.75MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.gate_proj        | \u001b[92m0.0002013939\u001b[0m | 108881      | 0.05000     | 1.362     | 0.619        | 18202.81MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.down_proj        | \u001b[92m0.0000316900\u001b[0m | 108881      | 0.05000     | 8.398     | 4.215        | 18421.73MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[92m0.0000186965\u001b[0m | 108881      | 0.05000     | 1.326     | 0.743        | 18820.81MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[92m0.0000109275\u001b[0m | 108881      | 0.05000     | 1.312     | 0.743        | 18772.27MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[92m0.0000978793\u001b[0m | 108881      | 0.05000     | 1.347     | 0.743        | 18744.98MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.o_proj     | \u001b[92m0.0000171202\u001b[0m | 108881      | 0.05000     | 1.369     | 0.488        | 18954.06MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.up_proj          | \u001b[92m0.0002305104\u001b[0m | 108881      | 0.05000     | 1.393     | 0.619        | 19330.97MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.gate_proj        | \u001b[92m0.0002200509\u001b[0m | 108881      | 0.05000     | 1.369     | 0.619        | 19285.94MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.down_proj        | \u001b[92m0.0000308212\u001b[0m | 108881      | 0.05000     | 8.462     | 4.216        | 19504.31MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[92m0.0000184943\u001b[0m | 108881      | 0.05000     | 1.351     | 0.743        | 19900.07MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[92m0.0000128183\u001b[0m | 108881      | 0.05000     | 1.317     | 0.743        | 19850.79MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[92m0.0001056705\u001b[0m | 108881      | 0.05000     | 1.354     | 0.743        | 19823.50MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.o_proj     | \u001b[92m0.0000107901\u001b[0m | 108881      | 0.05000     | 1.354     | 0.509        | 20032.35MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.up_proj          | \u001b[92m0.0002743698\u001b[0m | 108881      | 0.05000     | 1.391     | 0.619        | 20407.87MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.gate_proj        | \u001b[92m0.0002560611\u001b[0m | 108881      | 0.05000     | 1.370     | 0.619        | 20362.84MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.down_proj        | \u001b[92m0.0000438329\u001b[0m | 108881      | 0.05000     | 8.461     | 4.250        | 20582.20MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[92m0.0000148507\u001b[0m | 108881      | 0.05000     | 1.344     | 0.744        | 20978.07MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[92m0.0000149223\u001b[0m | 108881      | 0.05000     | 1.307     | 0.744        | 20928.70MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[92m0.0000915765\u001b[0m | 108881      | 0.05000     | 1.340     | 0.744        | 20901.41MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.o_proj     | \u001b[92m0.0000126092\u001b[0m | 108881      | 0.05000     | 1.362     | 0.499        | 21110.51MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.up_proj          | \u001b[92m0.0003010414\u001b[0m | 108881      | 0.05000     | 1.384     | 0.620        | 21484.96MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.gate_proj        | \u001b[92m0.0002764333\u001b[0m | 108881      | 0.05000     | 1.362     | 0.620        | 21439.93MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.down_proj        | \u001b[92m0.0000542299\u001b[0m | 108881      | 0.05000     | 8.487     | 4.228        | 21660.72MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[92m0.0000142301\u001b[0m | 108881      | 0.05000     | 1.347     | 0.742        | 22055.33MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[92m0.0000170495\u001b[0m | 108881      | 0.05000     | 1.296     | 0.742        | 22006.46MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[92m0.0001014641\u001b[0m | 108881      | 0.05000     | 1.349     | 0.742        | 21979.26MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.o_proj     | \u001b[92m0.0000164532\u001b[0m | 108881      | 0.05000     | 1.371     | 0.488        | 22188.01MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.up_proj          | \u001b[92m0.0003237349\u001b[0m | 108881      | 0.05000     | 1.396     | 0.620        | 22562.66MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.gate_proj        | \u001b[92m0.0003101829\u001b[0m | 108881      | 0.05000     | 1.402     | 0.620        | 22517.72MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.down_proj        | \u001b[92m0.0000527218\u001b[0m | 108881      | 0.05000     | 8.452     | 4.249        | 22737.55MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[92m0.0000150535\u001b[0m | 108881      | 0.05000     | 1.333     | 0.744        | 23135.28MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[92m0.0000189380\u001b[0m | 108881      | 0.05000     | 1.310     | 0.744        | 23086.59MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[92m0.0000967283\u001b[0m | 108881      | 0.05000     | 1.350     | 0.744        | 23059.30MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.o_proj     | \u001b[92m0.0000064371\u001b[0m | 108881      | 0.05000     | 1.349     | 0.513        | 23268.36MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.up_proj          | \u001b[92m0.0003767766\u001b[0m | 108881      | 0.05000     | 1.413     | 0.619        | 23641.79MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.gate_proj        | \u001b[92m0.0003602764\u001b[0m | 108881      | 0.05000     | 1.391     | 0.619        | 23596.76MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.down_proj        | \u001b[92m0.0000832023\u001b[0m | 108881      | 0.05000     | 8.475     | 4.230        | 23814.90MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[92m0.0000155091\u001b[0m | 108881      | 0.05000     | 1.339     | 0.743        | 24213.83MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[92m0.0000279224\u001b[0m | 108881      | 0.05000     | 1.315     | 0.743        | 24164.33MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[92m0.0001102170\u001b[0m | 108881      | 0.05000     | 1.339     | 0.743        | 24137.96MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.o_proj     | \u001b[92m0.0000180536\u001b[0m | 108881      | 0.05000     | 1.357     | 0.497        | 24346.66MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.up_proj          | \u001b[92m0.0004711756\u001b[0m | 108881      | 0.05000     | 1.392     | 0.619        | 24720.03MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.gate_proj        | \u001b[92m0.0004728316\u001b[0m | 108881      | 0.05000     | 1.385     | 0.619        | 24675.00MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.down_proj        | \u001b[92m0.0001048934\u001b[0m | 108881      | 0.05000     | 8.490     | 4.220        | 24892.13MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.k_proj     | \u001b[92m0.0000208805\u001b[0m | 108881      | 0.05000     | 1.340     | 0.743        | 25293.89MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.v_proj     | \u001b[92m0.0000455859\u001b[0m | 108881      | 0.05000     | 1.314     | 0.743        | 25244.77MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.q_proj     | \u001b[92m0.0001523212\u001b[0m | 108881      | 0.05000     | 1.342     | 0.743        | 25217.58MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.o_proj     | \u001b[92m0.0000232504\u001b[0m | 108881      | 0.05000     | 1.371     | 0.491        | 25424.46MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.up_proj          | \u001b[92m0.0006680471\u001b[0m | 108881      | 0.05000     | 1.420     | 0.620        | 25798.09MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.gate_proj        | \u001b[92m0.0006613598\u001b[0m | 108881      | 0.05000     | 1.362     | 0.620        | 25753.15MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.down_proj        | \u001b[92m0.0001583417\u001b[0m | 108881      | 0.05000     | 8.427     | 4.222        | 25969.09MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.k_proj     | \u001b[92m0.0000274195\u001b[0m | 108881      | 0.05000     | 1.338     | 0.742        | 26374.75MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.v_proj     | \u001b[92m0.0000643543\u001b[0m | 108881      | 0.05000     | 1.319     | 0.742        | 26325.87MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.q_proj     | \u001b[92m0.0001830609\u001b[0m | 108881      | 0.05000     | 1.348     | 0.742        | 26298.05MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.o_proj     | \u001b[92m0.0000584990\u001b[0m | 108881      | 0.05000     | 1.370     | 0.511        | 26503.15MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.up_proj          | \u001b[92m0.0008913524\u001b[0m | 108881      | 0.05000     | 1.396     | 0.620        | 26878.13MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.gate_proj        | \u001b[92m0.0009077674\u001b[0m | 108881      | 0.05000     | 1.366     | 0.620        | 26833.20MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.down_proj        | \u001b[92m0.0001741608\u001b[0m | 108881      | 0.05000     | 8.455     | 4.243        | 27049.75MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     | max_vram       |\n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.k_proj     | \u001b[92m0.0000225106\u001b[0m | 108881      | 0.05000     | 1.334     | 0.742        | 27455.23MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.v_proj     | \u001b[92m0.0000624057\u001b[0m | 108881      | 0.05000     | 1.313     | 0.742        | 27406.35MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.q_proj     | \u001b[92m0.0001652478\u001b[0m | 108881      | 0.05000     | 1.359     | 0.742        | 27379.67MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 24        | self_attn.o_proj     | \u001b[92m0.0000262652\u001b[0m | 108881      | 0.05000     | 1.404     | 0.510        | 27583.26MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 24        | mlp.up_proj          | \u001b[92m0.0009464314\u001b[0m | 108881      | 0.05000     | 1.397     | 0.619        | 27956.61MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\u001b[32mINFO\u001b[0m  | gptq        | 24        | mlp.gate_proj        | \u001b[92m0.0008946719\u001b[0m | 108881      | 0.05000     | 1.382     | 0.619        | 27912.42MB     | \n",
            "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Quantizing mlp.down_proj in layer     [24 of 27] | 0:10:13 / 0:11:26 [25/28] 89.3%"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacity of 39.56 GiB of which 794.88 MiB is free. Process 683883 has 38.77 GiB memory in use. Of the allocated memory 32.11 GiB is allocated by PyTorch, and 6.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-191668009>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# increase `batch_size` to match gpu/vram specs to speed up quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/models/base.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(self, calibration_dataset, calibration_dataset_concat_size, batch_size, calibration_enable_gpu_cache, tokenizer, logger_board, backend, buffered_fwd, auto_gc, adapter, adapter_calibration_dataset, calibration_data_min_length)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mmodule_looper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModuleLooper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         return module_looper.loop(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mcalibration_enable_gpu_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalibration_enable_gpu_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mbuffered_fwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffered_fwd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/looper/module_looper.py\u001b[0m in \u001b[0;36mloop\u001b[0;34m(self, auto_gc, calibration_enable_gpu_cache, buffered_fwd, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mname_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                             \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_gc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_gc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m                             \u001b[0mprocessed_subset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/looper/gptq_processor.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, module, auto_gc)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mwq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdamp_percent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/quantization/gptq.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(self, blocksize)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mHinv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/quantization/gptq.py\u001b[0m in \u001b[0;36mhessian_inverse\u001b[0;34m(self, H)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;31m# TODO call to torch.linalg is not threadsafe? Porque no? Esta muy mal.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mH2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mHinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcholesky_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.34 GiB. GPU 0 has a total capacity of 39.56 GiB of which 794.88 MiB is free. Process 683883 has 38.77 GiB memory in use. Of the allocated memory 32.11 GiB is allocated by PyTorch, and 6.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5V5eHS_wZmaZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}