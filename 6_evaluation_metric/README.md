# ImageÂ OCRÂ Benchmark 

> **Goal**Â Â Benchmark Qwenâ€‘2.5â€¯VL (base & LoRAâ€‘fineâ€‘tuned) against openâ€‘source PyTesseract on a Gujarati test set, evaluating Wordâ€‘, Characterâ€‘ and Layoutâ€‘level accuracy.

> ðŸ“„ **Detailed Report**: Please refer to [`ChatGPT_Generated_Evaluation_Report.md`](ChatGPT_Generated_Evaluation_Report.md) for an in-depth comparison of all results, metrics, and insights.

> The **'reference'** folder consist of the Ground Truth of the test data which will be used to evaluate the OCR outputs. 

---

## 1Â Â·Â KeyÂ Scripts

| Filename                      | Purpose                                                                                                                                                                                           |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`qwen2_5_vl_inference.py`** | Runs the Qwenâ€‘2.5â€¯VL visualâ€‘language model and saves a `.txt` file per input image.                                                                                                               |
| **`combine_evaluation.py`**   | Calculates **WER**, **CER** and **CustomÂ Structuralâ€¯Accuracy (SSA)** by comparing each OCR file with the groundâ€‘truth reference. Produces a consolidated Excel workbook in **`ocr_evaluation/`**. |
| **`generate_heatmap.py`**     | Reads any of the metric spreadsheets and creates an interactive Plotly heatmap (`.html`) for quick visual inspection.                                                                             |

---

## 2Â Â·Â ResultÂ Sets

| SystemÂ /Â Variant                | Folder                                            | Image Source        |
| ------------------------------- | ------------------------------------------------- | ------------------- |
| Qwenâ€¯2.5â€‘VLÂ **Base**Â (original) | `qwen25_vl_base_original_images_results`          | raw images          |
| Qwenâ€¯2.5â€‘VLÂ **Base**Â (enhanced) | `qwen25_vl_base_enhanced_images_results`          | restored / denoised |
| Qwenâ€¯2.5â€‘VLÂ **LoRA**Â (original) | `qwen25_vl_lora_finetune_original_images_results` | raw images          |
| Qwenâ€¯2.5â€‘VLÂ **LoRA**Â (enhanced) | `qwen25_vl_lora_finetune_enhanced_images_results` | restored / denoised |
| **PyTesseract** (OSS)           | `pytesseract_ocr_results`                         | baseline OCR        |

All five folders mirror the groundâ€‘truth naming scheme (e.g. `Swarupsannidhan_0081.txt`).

---

## 3Â Â·Â MetricÂ Computation

* **WER**Â â€“ WordÂ ErrorÂ Rate (0Â = perfect, >â€¯0.5Â = poor)
* **CER**Â â€“ CharacterÂ ErrorÂ Rate (<â€¯0.05Â = veryÂ high quality, >â€¯0.3Â = poor)
* **SSA**Â â€“ StructuralÂ SequenceÂ Accuracy (>â€¯0.95Â = near perfect, <â€¯0.80Â = major layout issues)

`combine_evaluation.py` traverses each result folder, aligns hypothesis to reference and appends fullâ€‘precision scores to **`ocr_evaluation/ocr_comparison_<folder>.xlsx`**.

---

## 4Â Â·Â Outputs

1. **Metric spreadsheets**Â Â Â `ocr_evaluation/ocr_evaluation_<model>.xlsx`
2. **Interactive heatmaps**Â Â `heatmaps/ocr_evaluation_<model>_heatmap.html`

> TipÂ Â Open the HTML files in any browser; hover a cell to inspect perâ€‘page metric values.

---

## 5Â Â·Â HowÂ toÂ Reproduce

```bash
# 1. Inference on each model variant
python qwen2_5_vl_inference.py 

# 2. Get combined evaluation of result type vs ground truth
python combine_evaluation.py 

# 3. Heatmap visualisation
python generate_heatmap.py 
```
